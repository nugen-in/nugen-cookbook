{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVMs4Y399o1-"
   },
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMYwmArPzWby"
   },
   "source": [
    "### **Generating Embeddings with Nugen API**\n",
    "\n",
    "This Colab notebook showcases how to extract information from Wikipedia, break it into smaller sections, and generate high-performance embeddings using the [Nugen API](https://docs.nugen.in/introduction)\n",
    "\n",
    "With Nugen’s cutting-edge API, you can easily generate embeddings that are optimized for speed and accuracy, enabling faster and more relevant results in your applications. Explore the power of Nugen’s domain-aligned models and elevate your AI-driven solutions with world-class performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FarRQSZFzi_Y"
   },
   "source": [
    "### **Setup**\n",
    "**Install Required Libraries**\n",
    "We'll install the required Python libraries to interact with Wikipedia, split sections, and count tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW40JqzAKCuC",
    "outputId": "3c838dee-5d57-48db-e2f6-e3c9bba959ad"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet mwclient mwparserfromhell pandas tiktoken openai requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bhcoVBn1B8W"
   },
   "source": [
    "**Import Necessary Libraries**\n",
    "\n",
    "These libraries help us work with Wikipedia articles, clean and process them, and prepare them for embedding using the Nugen API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kk-ug8DD1N7E"
   },
   "outputs": [],
   "source": [
    "import mwclient\n",
    "import mwparserfromhell\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1sn9yDQ1QHw"
   },
   "source": [
    "### **Access the Nugen API**\n",
    "\n",
    "**API Key Setup**\n",
    "\n",
    "First, we need to set up the Nugen API to generate embeddings. To do this, you'll need an API key from Nugen, which can be obtained for free by visiting [Nugen API Documentation](https://docs.nugen.in/). Once you have your API key, make sure to replace <your_api_key> in the code with the actual key you get from Nugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjFprmJc1YSV",
    "outputId": "777388ec-be79-4772-d6d1-2b22106eaa6a"
   },
   "outputs": [],
   "source": [
    "url_api_server = \"https://api.nugen.in/inference/embeddings\"\n",
    "api_key = \"<--API KEY-->\"  \n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjela3Yg15H1"
   },
   "source": [
    "### **Get Wikipedia Articles**\n",
    "**Choosing Wikipedia Articles**\n",
    "\n",
    "We are going to retrieve articles related to the 2022 Winter Olympics using a Wikipedia category. This section searches for all pages within that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-fOFKF6162k",
    "outputId": "c3bb815c-07be-484c-ed9d-358f27502790"
   },
   "outputs": [],
   "source": [
    "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
    "WIKI_SITE = \"en.wikipedia.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvNIQ3Ut2QLR"
   },
   "source": [
    "**Extract Article Titles**\n",
    "\n",
    "We now gather all the article titles under this category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjnqw5mR2DIW",
    "outputId": "60fcb86f-6cde-4a6e-96ca-ccf42a143261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 35 article titles for processing.\n"
     ]
    }
   ],
   "source": [
    "def titles_from_category(category, max_depth):\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "# Initialize the Wikipedia client\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "\n",
    "# Select 20% of the articles for processing you can modify this according to your use case.\n",
    "sample_size = int(0.2 * len(titles))\n",
    "sampled_titles = random.sample(list(titles), sample_size)\n",
    "print(f\"Selected {len(sampled_titles)} article titles for processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbTskCOa5Cy_"
   },
   "source": [
    "**How It Works**\n",
    "\n",
    "  1. **titles_from_category Function**: This function takes a Wikipedia category and retrieves all article titles within that category and its subcategories up to a specified depth.\n",
    "\n",
    "  2. **max_depth Parameter:** Controls how deep the function will go into subcategories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y78UsWuzACca"
   },
   "source": [
    "### **Chunk Documents**\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "\n",
    "* Discard less relevant-looking sections like External Links and Footnotes\n",
    "* Clean up the text by removing reference tags (e.g., ), whitespace, and super short sections\n",
    "* Split each article into sections\n",
    "* Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "* If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along     semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQ3jbtSw61eO",
    "outputId": "33bc17d3-861c-4ddc-fd9f-6ee6fe4c375b"
   },
   "outputs": [],
   "source": [
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "def all_subsections_from_section(section, parent_titles, sections_to_ignore):\n",
    "    \"\"\"Extract subsections from a Wikipedia section.\"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYv5v4X82gEl",
    "outputId": "b0aa84bf-18e7-4166-98ee-7ddcb5495eb1"
   },
   "outputs": [],
   "source": [
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIGilLid2i5l"
   },
   "source": [
    "This function splits the articles into smaller sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg9m0k0d2kM7"
   },
   "source": [
    "**Clean Up Sections**\n",
    "\n",
    "We clean the sections to remove unnecessary information, such as reference tags (<ref>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83hedxcv2jvk",
    "outputId": "45c80699-f4f1-4a87-a64d-93afa3c55003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1841 sections in 179 pages.\n"
     ]
    }
   ],
   "source": [
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 89 sections, leaving 1752 sections.\n",
      "['2022 Winter Olympics torch relay']\n",
      "{{Short description|none}}\n",
      "{{Use dmy dates|date=September 2021}}\n",
      "{{Infobox Ol...\n",
      "\n",
      "['2022 Winter Olympics torch relay', '==Relay==']\n",
      "Activists staged a protest at the Olympic torch lighting ceremony in Greece.\n",
      "...\n",
      "\n",
      "['2022 Winter Olympics torch relay', '==Route in Greece==']\n",
      "{{Location map+|Greece\n",
      "| width   = 450\n",
      "| float   = center\n",
      "| places  = \n",
      "{{Loca...\n",
      "\n",
      "['2022 Winter Olympics torch relay', '==Route in China==', '===Flame display leg (stage 1)===']\n",
      "{| class=wikitable\n",
      "!Division\n",
      "!Route\n",
      "!Map\n",
      "|-\n",
      "|[[Beijing]]\n",
      "|{{Collapsible list|...\n",
      "\n",
      "['2022 Winter Olympics torch relay', '==Route in China==', '===Beijing municipal leg (stages 2 and 3)===']\n",
      "{{Location map+|China Beijing municipality \n",
      "| width   = 450\n",
      "| float   = cente...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# Filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    titles, text = section\n",
    "    return len(text) >= 16\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n",
    "\n",
    "# Display example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    print(ws[1][:77] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6Wasvbi26gM"
   },
   "source": [
    "**Handle Text Length (Tokens)**\n",
    "\n",
    "Embeddings work best when the text is not too long. We count the tokens (words and characters) to ensure that each section is short enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Us1DUzXQ26Bq",
    "outputId": "1cfee394-935d-4480-be08-3fd6e429563a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752 Wikipedia sections split into 2062 strings.\n"
     ]
    }
   ],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # Just to select tokenizer, does not use OpenAI models\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]\n",
    "    total_tokens = num_tokens(string)\n",
    "    halfway = total_tokens // 2\n",
    "    best_diff = halfway\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        left = delimiter.join(chunks[: i + 1])\n",
    "        left_tokens = num_tokens(left)\n",
    "        diff = abs(halfway - left_tokens)\n",
    "        if diff >= best_diff:\n",
    "            break\n",
    "        best_diff = diff\n",
    "    left = delimiter.join(chunks[:i])\n",
    "    right = delimiter.join(chunks[i:])\n",
    "    return [left, right]\n",
    "\n",
    "def truncated_string(string: str, model: str, max_tokens: int, print_warning: bool = True) -> str:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "def split_strings_from_subsection(subsection: tuple[list[str], str], max_tokens: int = 1000, model: str = GPT_MODEL, max_recursion: int = 5) -> list[str]:\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    if num_tokens(string) <= max_tokens:\n",
    "        return [string]\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "        left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "        if left == \"\" or right == \"\":\n",
    "            continue\n",
    "        results = []\n",
    "        for half in [left, right]:\n",
    "            half_subsection = (titles, half)\n",
    "            half_strings = split_strings_from_subsection(\n",
    "                half_subsection,\n",
    "                max_tokens=max_tokens,\n",
    "                model=model,\n",
    "                max_recursion=max_recursion - 1,\n",
    "            )\n",
    "            results.extend(half_strings)\n",
    "        return results\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "\n",
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_bTrfJk3Fto"
   },
   "source": [
    "### **Generate Embeddings**\n",
    "**Prepare Text for Embedding**\n",
    "\n",
    "After splitting the sections, we convert them into numerical values (embeddings). These embeddings help computers understand the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRUXNpud3PsF",
    "outputId": "4615aa0b-3c4f-449e-df61-128ecb5c95cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752 Wikipedia sections split into 2062 strings.\n",
      "Processing batch 0 to 99\n",
      "Processing batch 100 to 199\n",
      "Processing batch 200 to 299\n",
      "Processing batch 300 to 399\n",
      "Processing batch 400 to 499\n",
      "Processing batch 500 to 599\n",
      "Processing batch 600 to 699\n",
      "Processing batch 700 to 799\n",
      "Processing batch 800 to 899\n",
      "Processing batch 900 to 999\n",
      "Processing batch 1000 to 1099\n",
      "Processing batch 1100 to 1199\n",
      "Processing batch 1200 to 1299\n",
      "Processing batch 1300 to 1399\n",
      "Processing batch 1400 to 1499\n",
      "Processing batch 1500 to 1599\n",
      "Processing batch 1600 to 1699\n",
      "Processing batch 1700 to 1799\n",
      "Processing batch 1800 to 1899\n",
      "Processing batch 1900 to 1999\n",
      "Processing batch 2000 to 2099\n"
     ]
    }
   ],
   "source": [
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n",
    "\n",
    "# Fetch embeddings from Nugen API\n",
    "BATCH_SIZE = 100\n",
    "EMBEDDING_MODEL = \"nugen-flash-embed\"\n",
    "embeddings = []\n",
    "\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Processing batch {batch_start} to {batch_end-1}\")\n",
    "\n",
    "    payload = {\n",
    "        \"input\": batch,\n",
    "        \"model\": EMBEDDING_MODEL\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url_api_server, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        batch_embeddings = [e['embedding'] for e in data['data']]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred: {e}\")\n",
    "        print(f\"Response content: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXpyARru3R-u"
   },
   "source": [
    "### **Save the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-QS2qxy3cEH",
    "outputId": "da2dfe5e-b777-432a-eecc-8344fec85c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to winter_olympics_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the embeddings\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n",
    "SAVE_PATH = \"winter_olympics_2022.csv\"\n",
    "df.to_csv(SAVE_PATH, index=False)\n",
    "print(f\"Embeddings saved to {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
