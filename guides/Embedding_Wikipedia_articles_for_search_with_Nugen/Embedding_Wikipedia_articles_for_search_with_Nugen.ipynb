{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVMs4Y399o1-"
   },
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMYwmArPzWby"
   },
   "source": [
    "# **Generating Embeddings with Nugen API**\n",
    "\n",
    "This Colab notebook helps you extract information from Wikipedia, split it into smaller sections, and generate numerical embeddings using the Nugen API. Embeddings are useful in many applications like search engines and recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOOm7zJv4zR2"
   },
   "source": [
    "**Code Overview**\n",
    "The code is divided into several main parts:\n",
    "\n",
    "1. Fetching Wikipedia Article Titles\n",
    "2. Processing and Cleaning Wikipedia Articles\n",
    "3. Splitting Text for API Calls\n",
    "4. Making API Calls to Nugen\n",
    "5. Saving Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FarRQSZFzi_Y"
   },
   "source": [
    "**1. Setup**\n",
    "##Install Required Libraries\n",
    "We'll install the required Python libraries to interact with Wikipedia, split sections, and count tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW40JqzAKCuC",
    "outputId": "3c838dee-5d57-48db-e2f6-e3c9bba959ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mwclient\n",
      "  Downloading mwclient-0.11.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.46.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from mwclient) (1.3.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->mwclient) (3.2.2)\n",
      "Downloading mwclient-0.11.0-py3-none-any.whl (33 kB)\n",
      "Downloading mwparserfromhell-0.6.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (191 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.46.0-py3-none-any.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.0/375.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mwparserfromhell, jiter, h11, tiktoken, httpcore, mwclient, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 mwclient-0.11.0 mwparserfromhell-0.6.6 openai-1.46.0 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mwclient mwparserfromhell pandas tiktoken openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bhcoVBn1B8W"
   },
   "source": [
    "#**Import Necessary Libraries**\n",
    "These libraries help us work with Wikipedia articles, clean and process them, and prepare them for embedding using the Nugen API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kk-ug8DD1N7E"
   },
   "outputs": [],
   "source": [
    "import mwclient\n",
    "import mwparserfromhell\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1sn9yDQ1QHw"
   },
   "source": [
    "#**2. Access the Nugen API**\n",
    "\n",
    "##API Key Setup\n",
    "First, we need to set up the Nugen API to generate embeddings. You’ll need an API key from Nugen, which should be stored as an environment variable. You can replace <your_api_key> with your actual key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjFprmJc1YSV",
    "outputId": "777388ec-be79-4772-d6d1-2b22106eaa6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "url_api_server = \"https://api.nugen.in/inference/embeddings\"\n",
    "api_key = <GET YOUR NUGEN API KEYS>  # Replace with your API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjela3Yg15H1"
   },
   "source": [
    "#**3. Get Wikipedia Articles**\n",
    "##**Choosing Wikipedia Articles**\n",
    "We are going to retrieve articles related to the 2022 Winter Olympics using a Wikipedia category. This section searches for all pages within that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-fOFKF6162k",
    "outputId": "c3bb815c-07be-484c-ed9d-358f27502790"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
    "WIKI_SITE = \"en.wikipedia.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvNIQ3Ut2QLR"
   },
   "source": [
    "##**Extract Article Titles**\n",
    "We now gather all the article titles under this category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjnqw5mR2DIW",
    "outputId": "60fcb86f-6cde-4a6e-96ca-ccf42a143261"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 35 article titles for processing.\n"
     ]
    }
   ],
   "source": [
    "def titles_from_category(category, max_depth):\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "# Initialize the Wikipedia client\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "\n",
    "# Select 20% of the articles for processing\n",
    "sample_size = int(0.2 * len(titles))\n",
    "sampled_titles = random.sample(list(titles), sample_size)\n",
    "print(f\"Selected {len(sampled_titles)} article titles for processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbTskCOa5Cy_"
   },
   "source": [
    "##**How It Works**\n",
    "  1. **titles_from_category Function**: This function takes a Wikipedia category and retrieves all article titles within that category and its subcategories up to a specified depth.\n",
    "\n",
    "  2. **max_depth Parameter:** Controls how deep the function will go into subcategories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y78UsWuzACca"
   },
   "source": [
    "#**4. Chunk Documents**\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "\n",
    "1. Discard less relevant-looking sections like External Links and Footnotes\n",
    "2. Clean up the text by removing reference tags (e.g., ), whitespace, and super short sections\n",
    "3. Split each article into sections\n",
    "4. Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "5. If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQ3jbtSw61eO",
    "outputId": "33bc17d3-861c-4ddc-fd9f-6ee6fe4c375b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "def all_subsections_from_section(section, parent_titles, sections_to_ignore):\n",
    "    \"\"\"Extract subsections from a Wikipedia section.\"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYv5v4X82gEl",
    "outputId": "b0aa84bf-18e7-4166-98ee-7ddcb5495eb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIGilLid2i5l"
   },
   "source": [
    "This function splits the articles into smaller sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg9m0k0d2kM7"
   },
   "source": [
    "##**Clean Up Sections**\n",
    "We clean the sections to remove unnecessary information, such as reference tags (<ref>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83hedxcv2jvk",
    "outputId": "45c80699-f4f1-4a87-a64d-93afa3c55003"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1841 sections in 179 pages.\n",
      "Filtered out 89 sections, leaving 1752 sections.\n",
      "['Haiti at the 2022 Winter Olympics']\n",
      "{{infobox country at games\n",
      "|NOC = HAI\n",
      "|NOCname = Comité Olympique Haïtien\n",
      "|ga...\n",
      "\n",
      "['Haiti at the 2022 Winter Olympics', '==Competitors==']\n",
      "The following is the list of number of competitors at the Games per sport/dis...\n",
      "\n",
      "['Haiti at the 2022 Winter Olympics', '==Alpine skiing==']\n",
      "{{main article|Alpine skiing at the 2022 Winter Olympics|Alpine skiing at the...\n",
      "\n",
      "['Alpine skiing at the 2022 Winter Olympics']\n",
      "{{short description|Sport at the 2022 Winter Olympic}}\n",
      "{{Use dmy dates|date=F...\n",
      "\n",
      "['Alpine skiing at the 2022 Winter Olympics', '==Qualification==']\n",
      "{{Main|Alpine skiing at the 2022 Winter Olympics – Qualification}}\n",
      "A maximum ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")\n",
    "\n",
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# Filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    titles, text = section\n",
    "    return len(text) >= 16\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n",
    "\n",
    "# Display example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    print(ws[1][:77] + \"...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6Wasvbi26gM"
   },
   "source": [
    "#**5. Handle Text Length (Tokens)**\n",
    "Embeddings work best when the text is not too long. We count the tokens (words and characters) to ensure that each section is short enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Us1DUzXQ26Bq",
    "outputId": "1cfee394-935d-4480-be08-3fd6e429563a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752 Wikipedia sections split into 2062 strings.\n"
     ]
    }
   ],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # Just to select tokenizer, does not use OpenAI models\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]\n",
    "    total_tokens = num_tokens(string)\n",
    "    halfway = total_tokens // 2\n",
    "    best_diff = halfway\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        left = delimiter.join(chunks[: i + 1])\n",
    "        left_tokens = num_tokens(left)\n",
    "        diff = abs(halfway - left_tokens)\n",
    "        if diff >= best_diff:\n",
    "            break\n",
    "        best_diff = diff\n",
    "    left = delimiter.join(chunks[:i])\n",
    "    right = delimiter.join(chunks[i:])\n",
    "    return [left, right]\n",
    "\n",
    "def truncated_string(string: str, model: str, max_tokens: int, print_warning: bool = True) -> str:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "def split_strings_from_subsection(subsection: tuple[list[str], str], max_tokens: int = 1000, model: str = GPT_MODEL, max_recursion: int = 5) -> list[str]:\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    if num_tokens(string) <= max_tokens:\n",
    "        return [string]\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "        left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "        if left == \"\" or right == \"\":\n",
    "            continue\n",
    "        results = []\n",
    "        for half in [left, right]:\n",
    "            half_subsection = (titles, half)\n",
    "            half_strings = split_strings_from_subsection(\n",
    "                half_subsection,\n",
    "                max_tokens=max_tokens,\n",
    "                model=model,\n",
    "                max_recursion=max_recursion - 1,\n",
    "            )\n",
    "            results.extend(half_strings)\n",
    "        return results\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "\n",
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_bTrfJk3Fto"
   },
   "source": [
    "#**6. Generate Embeddings**\n",
    "##**Prepare Text for Embedding**\n",
    "After splitting the sections, we convert them into numerical values (embeddings). These embeddings help computers understand the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRUXNpud3PsF",
    "outputId": "4615aa0b-3c4f-449e-df61-128ecb5c95cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752 Wikipedia sections split into 2062 strings.\n",
      "Processing batch 0 to 99\n",
      "Processing batch 100 to 199\n",
      "Processing batch 200 to 299\n",
      "Processing batch 300 to 399\n",
      "Processing batch 400 to 499\n",
      "Processing batch 500 to 599\n",
      "Processing batch 600 to 699\n",
      "Processing batch 700 to 799\n",
      "Processing batch 800 to 899\n",
      "Processing batch 900 to 999\n",
      "Processing batch 1000 to 1099\n",
      "Processing batch 1100 to 1199\n",
      "Processing batch 1200 to 1299\n",
      "Processing batch 1300 to 1399\n",
      "Processing batch 1400 to 1499\n",
      "Processing batch 1500 to 1599\n",
      "Processing batch 1600 to 1699\n",
      "Processing batch 1700 to 1799\n",
      "Processing batch 1800 to 1899\n",
      "Processing batch 1900 to 1999\n",
      "Processing batch 2000 to 2099\n"
     ]
    }
   ],
   "source": [
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n",
    "\n",
    "# Fetch embeddings from Nugen API\n",
    "BATCH_SIZE = 100\n",
    "EMBEDDING_MODEL = \"nugen-flash-embed\"\n",
    "embeddings = []\n",
    "\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Processing batch {batch_start} to {batch_end-1}\")\n",
    "\n",
    "    payload = {\n",
    "        \"input\": batch,\n",
    "        \"model\": EMBEDDING_MODEL\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url_api_server, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        batch_embeddings = [e['embedding'] for e in data['data']]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred: {e}\")\n",
    "        print(f\"Response content: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXpyARru3R-u"
   },
   "source": [
    "#**7. Save the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-QS2qxy3cEH",
    "outputId": "da2dfe5e-b777-432a-eecc-8344fec85c1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to winter_olympics_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the embeddings\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n",
    "SAVE_PATH = \"winter_olympics_2022.csv\"\n",
    "df.to_csv(SAVE_PATH, index=False)\n",
    "print(f\"Embeddings saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4R44LMHUWTF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
