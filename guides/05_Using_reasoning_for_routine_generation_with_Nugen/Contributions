# Contribution to the Original Code

## Overview
The following changes have been made to the original code to enhance the generation and validation of routines from help center articles. These contributions introduce **Chain of Thought (CoT)** for policy-to-routine transformation and a **Routine Quality Assurance (QA) Validator** to ensure the generated routines are accurate, compliant, and clear.

## Contribution: LLM-Based Routine Generator from Help Center Articles

### Overview

This contribution adds a scalable, automated system that:

- Loads help center articles from a CSV file.
- Converts policies into internal routines using an LLM (via Nugen API).
- Validates the routines for legal and procedural correctness.
- Displays results in a user-friendly HTML table.

---

### Key Additions

#### 1. Chain of Thought (CoT) for Routine Generation

The `generate_routine` function uses a **Chain of Thought (CoT)** prompting approach. This ensures that policies are:

- Thoroughly reviewed
- Translated into logically ordered, executable routines
- Structured with "if...then...else" conditions
- Augmented with external function calls when necessary
- Compliant with legal and privacy regulations
- Robust against edge cases and exceptions

This approach is implemented via a clearly defined `CONVERSION_PROMPT`, which guides the LLM to produce structured, programmatically usable output.

#### 2. Routine QA Validator

A validation step ensures that each generated routine is:

- **Complete**: Covers all aspects of the policy
- **Legally Compliant**: Adheres to relevant regulations like HIPAA, GDPR, PCI DSS
- **Accurate**: Faithfully represents the original policy
- **Clear**: Easy to interpret and implement

This is handled by the `evaluate_routine_for_quality` function, which performs an additional LLM inference for review.

#### 3. Parallel Processing for Article Handling

To improve performance, help center articles are processed in parallel using `ThreadPoolExecutor`, allowing multiple routines to be generated and validated concurrently. This significantly speeds up batch processing of large policy datasets.

#### 4. Display Formatted DataFrame

The final results — including the original policy, generated routine, and validation status — are presented using a styled Pandas DataFrame rendered in HTML. This improves readability and stakeholder visibility.

---

### Components

1. **Nugen API Client**
   - Encapsulates API calls to `/inference/completions`
   - Supports parameters like `model`, `prompt`, `temperature`, and `max_tokens`

2. **Chain of Thought Prompting**
   - Logical, step-by-step transformation of policy text into an executable format

3. **Validator Prompt**
   - Ensures quality of the LLM-generated routine via a second-pass evaluation

4. **Article Pipeline**
   - Loads policies from `helpcenter_articles.csv`
   - Applies both generation and validation steps in parallel

5. **UI Output**
   - Renders results in a clean, HTML-format table for easy review

---

### Improvements

| Area          | Before                           | After                                                                 |
|---------------|----------------------------------|------------------------------------------------------------------------|
| API Usage     | Not implemented                  | Introduced `NugenAPIClient` for clean and modular API interaction     |
| Prompting     | Absent                           | Defined detailed CoT and QA prompts for structured output             |
| Execution     | Single-threaded or manual        | Integrated `ThreadPoolExecutor` for parallelism                       |
| Validation    | None                             | Added validator model to ensure output integrity                      |
| Output        | Plain or unformatted             | HTML-rendered Pandas DataFrame for better visibility                  |
| Reliability   | Low                              | Extensive `try-except` handling added throughout                      |
| Modularity    | Minimal                          | Functions abstracted for reusability and testing                      |

---

### Example Prompts and Snippets

#### Chain of Thought Conversion Prompt

```python
CONVERSION_PROMPT = """
You are a helpful assistant tasked with taking an external facing help center article 
and converting it into an internal-facing programmatically executable routine optimized for an LLM. 
The LLM using this routine will be tasked with reading the policy, answering incoming questions 
from customers, and helping drive the case toward resolution.

Please follow these instructions:
1. Review the customer service policy carefully to ensure every step is accounted for.
2. Organize the instructions into a logical, step-by-step order, using the specified format.
3. Use the following format:
   - Main actions are numbered (e.g., 1, 2, 3).
   - Sub-actions are lettered under their relevant main actions (e.g., 1a, 1b).
   - Specify conditions using clear 'if...then...else' statements.
   - Actions requiring external systems should call a function (e.g., call check_delivery_date).
   - Prioritize compliance by adhering to privacy regulations and policies.
   - Handle exceptions by specifying steps for non-standard scenarios.

Important: If uncertain, respond with "I don't know."
"""
