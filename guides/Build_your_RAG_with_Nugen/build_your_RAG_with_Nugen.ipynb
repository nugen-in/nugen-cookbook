{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRTPCWWrpu2p"
   },
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAA-Q9eJg92j"
   },
   "source": [
    "## **Chat-with-PDF using Nugen APIs**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gBJgQLShhMd"
   },
   "source": [
    "This documentation explains the implementation of a chat-with-PDF functionality, where PDF documents are embedded into a vector database, and queries are answered based on contextual search from these embeddings. The code uses Nugen APIs for generating embeddings and language model completions, and Qdrant as the vector database to store and retrieve these embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5DjVs0Fh5qt"
   },
   "source": [
    "### Setup and Configuration\n",
    "Importing Libraries and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E88t43DJIPEu",
    "outputId": "0a2a4e97-c44a-4609-91f4-c5049cd7d513"
   },
   "outputs": [],
   "source": [
    "!pip install pymupdf\n",
    "!pip install qdrant-client\n",
    "!pip install chainlit\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "import chainlit as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKcdMb5iiESI"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "\n",
    "* **os and sys:** These modules are used to interact with the system environment and handle operations such as reading environment variables and exiting the program.\n",
    "\n",
    "* **hashlib:** Utilized for generating a unique hash of the PDF files to check for duplicates in the database.\n",
    "\n",
    "* **request** : For making API calls to Nugen's language model and embedding API.\n",
    "* **fitz (PyMuPDF):** A library for reading and extracting text from PDF files.\n",
    "\n",
    "* **QdrantClient:** A client to connect and interact with the Qdrant vector database, where embeddings are stored.\n",
    "* **dotenv:** Loads environment variables from a .env file to securely manage API keys and database URLs.\n",
    "\n",
    "* **chainlit:** Used to interact with users and manage messages within a chat-like interface.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpCXACJXjVd7"
   },
   "source": [
    "### **Defining Global Variables and Model Configuration**\n",
    "\n",
    "To get started with Nugen APIs, you'll need your Nugen API key, which you can obtain for free. Visit the \n",
    "[Nugen API Documentation](https://docs.nugen.in/api-reference/endpoint/embeddings/embeddings) to generate your key. Nugen offers free access to its powerful AI models, allowing you to integrate features like embeddings and language model completions at no cost.\n",
    "\n",
    "By leveraging this API key, you can seamlessly integrate Nugen’s cutting-edge APIs into your applications and start building advanced AI-powered solutions right away!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOq_-edWjnuz"
   },
   "outputs": [],
   "source": [
    "USE_API_PROVIDER = \"NUGEN\"\n",
    "if USE_API_PROVIDER == \"NUGEN\":\n",
    "    NUGEN_API_KEY = \"GET YOUR NUGEN API KEYS\"\n",
    "    LLM_API_URL = \"https://api.nugen.in/inference\"\n",
    "    model_llm = \"nugen-flash-instruct\"\n",
    "    model_embed = \"nugen-flash-embed\"\n",
    "    EMBED_DIMENSION = 768\n",
    "    EMBED_CHUNK_SIZE = int(EMBED_DIMENSION * 0.95)\n",
    "    EMBED_CHUNK_OVERLAP = int(EMBED_CHUNK_SIZE * 0.10)\n",
    "    LLM_API_PROVIDER_KEY = NUGEN_API_KEY\n",
    "else:\n",
    "    print(\"Unexpected USE_API_PROVIDER=\", USE_API_PROVIDER)\n",
    "    sys.exit()\n",
    "\n",
    "qdrant_client = QdrantClient(os.getenv(\"QDRANT_CLIENT_URL\"))\n",
    "collection_name = \"pdf_embeddings\"\n",
    "top_k = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs_UNbjkjpG9"
   },
   "source": [
    "### **USE API PROVIDER:**\n",
    "\n",
    "This variable determines which provider's API will be used. In this case, it is set to \"NUGEN\", so all API calls are directed to Nugen’s services.\n",
    "\n",
    "### Nugen API Configuration:\n",
    "\n",
    "\n",
    "*   **NUGEN_API_KEY:** API key for **Nugen's domain-aligned model services**, loaded from environment variables.\n",
    "*   **LLM_API_URL:** The endpoint for Nugen’s large language model inference API.\n",
    "*   **model_llm and model_embed:** These specify which models to use for instruction-based completion and text embeddings.\n",
    "      \n",
    "        1. model_llm: nugen-flash-instruct (used for answering user queries).\n",
    "        2. model_embed: nugen-flash-embed (used for generating embeddings from text).\n",
    "    \n",
    "### Embedding Parameters:\n",
    "\n",
    "*   **EMBED_DIMENSION:** Dimension of the embedding vector (768 for Nugen's embeddings).\n",
    "\n",
    "*  **EMBED_CHUNK_SIZE and EMBED_CHUNK_OVERLAP:** These control how PDF content is split into chunks for embedding. A chunk is the amount of text processed together, and overlap ensures continuity between adjacent chunks.\n",
    "\n",
    "**QdrantClient:** The client object for connecting to Qdrant (the vector database where embeddings are stored). It connects using the URL provided by the environment variable QDRANT_CLIENT_URL.\n",
    "\n",
    "**Collection Name:** collection_name is set to pdf_embeddings. This is the Qdrant collection where embeddings related to the PDFs will be stored.\n",
    "\n",
    "**top_k:** Defines the number of top results to retrieve from the Qdrant database when searching for relevant context based on the user query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALDirP5xm7C0"
   },
   "source": [
    "### **Setting up the Qdrant Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctWJ9Cbnm86m"
   },
   "outputs": [],
   "source": [
    "def setup_qdrant_collection(qdrant_client, collection_name, embed_dim):\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections().collections\n",
    "        if collection_name not in [collection.name for collection in collections]:\n",
    "            qdrant_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(size=embed_dim, distance=\"Cosine\")\n",
    "            )\n",
    "            print(f\"Collection '{collection_name}' created.\")\n",
    "        else:\n",
    "            print(f\"Collection '{collection_name}' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Qdrant collection: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaF13LcTnBWa"
   },
   "source": [
    "* This function checks if a collection (i.e., a \"bucket\" for storing embeddings) already exists in Qdrant.\n",
    "\n",
    "* If the collection does not exist, it creates a new one with vector size (embed_dim) based on the embedding dimensions of the Nugen model.\n",
    "\n",
    "* Cosine distance is used as the metric for comparing vectors, which is standard for similarity searches.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLp3FAeWnYky"
   },
   "source": [
    "### **Extracting Text and Splitting PDF into Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FThcKkDDnc_2"
   },
   "outputs": [],
   "source": [
    "def pdf_to_text_chunks(pdf_path, chunk_size, overlap_size):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\".join([page.get_text() for page in doc])\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap_size)]\n",
    "    print(text, chunks)\n",
    "    return text, chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otR4YGYxnhgZ"
   },
   "source": [
    "* This function opens the PDF using PyMuPDF (fitz) and extracts all the text from each page of the document.\n",
    "* Read more about pymupdf [here](https://pymupdf.readthedocs.io/)\n",
    "* The entire text is then split into chunks of a specific size (chunk_size) with some overlap (overlap_size). Overlapping chunks help maintain continuity in embeddings, which can improve retrieval performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQHTlXI0np_6"
   },
   "source": [
    "### **Generating Embeddings for Text Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DCJaWpOnv_y"
   },
   "outputs": [],
   "source": [
    "def create_embedding(text, model_embed):\n",
    "    url = f\"{LLM_API_URL}/embeddings\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_PROVIDER_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model_embed,\n",
    "        \"input\": text\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70pRPutUnyA5"
   },
   "source": [
    "* This function calls the Nugen embedding API to generate embeddings for the given text.\n",
    "* It sends a POST request to Nugen’s /embeddings endpoint with the text data and embedding model (model_embed).\n",
    "* The function returns the vector embedding of the text, which is later stored in Qdrant.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upcM9hk3oHz4"
   },
   "source": [
    "### **Storing Embeddings in Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0ma4MISoJzK"
   },
   "outputs": [],
   "source": [
    "def store_embeddings(chunks, file_path, file_hash, user_id, thread_id, message_id, collection_name):\n",
    "    try:\n",
    "        points = [\n",
    "            PointStruct(id=i, vector=create_embedding(chunk, model_embed), payload={\n",
    "                \"chunk_text\": chunk, \"file_path\": file_path, \"file_hash\": file_hash,\n",
    "                \"user_id\": user_id, \"thread_id\": thread_id, \"message_id\": message_id\n",
    "            }) for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "        print(\"Embeddings stored successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDnD7wb1oMnZ"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "* Purpose: This function chunks the input text and stores each chunk's embedding in Qdrant.\n",
    "\n",
    "* Key Parameters:\n",
    "  * chunks: List of text chunks to be embedded.\n",
    "  * file_path: Path of the file being processed.\n",
    "  * file_hash: Hash of the file to uniquely identify it.\n",
    "  * collection_name: Qdrant collection to store the embeddings.\n",
    "\n",
    "* Return: No return value, but embeddings are upserted (inserted or updated) into Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjHLFewCoMgo"
   },
   "source": [
    "### **Retrieving Relevant Context from Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc8AmM9lorub"
   },
   "outputs": [],
   "source": [
    "def simple_rag_retrieve(query, top_k, user_id, thread_id, collection_name):\n",
    "    try:\n",
    "        query_embedding = create_embedding(query, model_embed)\n",
    "        search_result = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=top_k,\n",
    "            query_filter={\"must\": [{\"key\": \"user_id\", \"match\": {\"value\": user_id}}]}\n",
    "        )\n",
    "        return \"\\n\".join([hit.payload['chunk_text'] for hit in search_result])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving context: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItMu2v0RotvH"
   },
   "source": [
    "* Purpose: This function searches Qdrant to retrieve the most  relevant text chunks for a given query.\n",
    "\n",
    "* Key Parameters:\n",
    "    *   query: The question or input from the user.\n",
    "    *   collection_name: Qdrant collection to search.\n",
    "    *   top_k: The number of top results to return.\n",
    "\n",
    "* Return: The retrieved context (relevant text chunks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KspIXQWQpNPH"
   },
   "source": [
    "### **Generating a Response Using Retrieved Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S5JoBqVpP58"
   },
   "outputs": [],
   "source": [
    "def generate_llm_response(context, query, model_llm):\n",
    "    url = f\"{LLM_API_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_PROVIDER_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model_llm,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Answer the question: {query}\"}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru0B2mG4pVPT"
   },
   "source": [
    "* This function sends a POST request to the Nugen API to generate a response based on the retrieved context and the user query.\n",
    "\n",
    "* **messages:** The request includes the context retrieved from Qdrant and the user’s query. The assistant uses these messages to generate a relevant response\n",
    "\n",
    "* The Nugen API processes this request and returns a completion (answer) that is sent back to the user.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l13bHML67GHG"
   },
   "outputs": [],
   "source": [
    "def embed_pdf(file_path, user_id, thread_id, message_id, collection_name):\n",
    "    url = \"https://api.nugen.in/inference/embeddings\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        payload = {\n",
    "            \"input\": f.read().decode(),  # Assuming the file is readable as text\n",
    "            \"model\": \"nugen-flash-embed\",\n",
    "            \"dimensions\": 123  # Set to your required dimensions\n",
    "        }\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer <token>\",  # Replace <token> with your actual API token\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        file_hash = data['file_hash']  # Adjust based on your API response structure\n",
    "        file_chunks_count = data['chunks_count']  # Adjust based on your API response structure\n",
    "        return True, file_hash, file_chunks_count\n",
    "    else:\n",
    "        return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuWzWs-H6qbF"
   },
   "outputs": [],
   "source": [
    "def simple_rag_generate(user_query, query_context):\n",
    "    url = \"https://api.nugen.in/inference/completions\"\n",
    "    payload = {\n",
    "        \"max_tokens\": 400,\n",
    "        \"model\": \"nugen-flash-instruct\",\n",
    "        \"prompt\": user_query + \" \" + (query_context or \"\"),\n",
    "        \"temperature\": 1\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer <token>\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['completion']  # Adjust based on API response structure\n",
    "    else:\n",
    "        return \"Error generating response.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTS2Hjf7pnno"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "By following this structure, the model enables users to upload PDFs, extract meaningful information from them, and ask questions that are answered based on the embedded content in the document. All of this is powered by Nugen’s APIs and the Qdrant vector database for high-quality search and retrieval.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
