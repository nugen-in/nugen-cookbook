{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVMs4Y399o1-"
   },
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention! To learn more, visit [Nugen](https://docs.nugen.in/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMYwmArPzWby"
   },
   "source": [
    "### **Generating Embeddings with Nugen API**\n",
    "\n",
    "This lesson demonstrates how to generate embeddings for texts using Nugen embeddings APIs. To do that, we will be following the steps mentioned below:\n",
    "1. Extract information from Wikipedia\n",
    "2. Break it into smaller sections\n",
    "3. Generate high-performance embeddings using the [Nugen API](https://docs.nugen.in/introduction)\n",
    "\n",
    "\n",
    "With Nugen’s cutting-edge API, you can easily generate embeddings that are optimized for speed and accuracy, enabling faster and more relevant results in your applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FarRQSZFzi_Y"
   },
   "source": [
    "### **Setup**\n",
    "**Install Required Libraries**\n",
    "We'll install the required Python libraries to interact with Wikipedia, split sections, and count tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet mwclient==0.11.0 mwparserfromhell==0.6.6 pandas==1.5.3 tiktoken==0.7.0 openai==1.34.0 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bhcoVBn1B8W"
   },
   "source": [
    "**Import Necessary Libraries**\n",
    "\n",
    "These libraries help us work with Wikipedia articles, clean and process them, and prepare them for embedding using the Nugen API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kk-ug8DD1N7E"
   },
   "outputs": [],
   "source": [
    "import mwclient\n",
    "import mwparserfromhell\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1sn9yDQ1QHw"
   },
   "source": [
    "### **Access the Nugen API**\n",
    "\n",
    "**API Key Setup**\n",
    "\n",
    "First, we need to set up the Nugen API to generate embeddings. To do this, you'll need an API key from Nugen. To access free API keys, you can visit [Nugen Dashboard](https://nugen-platform-frontend.azurewebsites.net/dashboard) Once you have your API key, make sure to replace <your_api_key> in the code with the actual key you get from Nugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjFprmJc1YSV",
    "outputId": "777388ec-be79-4772-d6d1-2b22106eaa6a"
   },
   "outputs": [],
   "source": [
    "url_api_server = \"https://api.nugen.in/inference/embeddings\"\n",
    "api_key = <enter your api key>\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjela3Yg15H1"
   },
   "source": [
    "### **Get Wikipedia Articles**\n",
    "**Choosing Wikipedia Articles**\n",
    "\n",
    "We are going to retrieve articles related to the 2022 Winter Olympics using a Wikipedia category. This section searches for all pages within that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-fOFKF6162k",
    "outputId": "c3bb815c-07be-484c-ed9d-358f27502790"
   },
   "outputs": [],
   "source": [
    "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
    "WIKI_SITE = \"en.wikipedia.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvNIQ3Ut2QLR"
   },
   "source": [
    "**Extract Article Titles**\n",
    "\n",
    "We now gather all the article titles under this category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjnqw5mR2DIW",
    "outputId": "60fcb86f-6cde-4a6e-96ca-ccf42a143261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 35 article titles for processing.\n"
     ]
    }
   ],
   "source": [
    "def titles_from_category(category, max_depth):\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "# Initialize the Wikipedia client\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "\n",
    "# Select 20% of the articles for processing you can modify this according to your use case.\n",
    "sample_size = int(0.2 * len(titles))\n",
    "sampled_titles = random.sample(list(titles), sample_size)\n",
    "print(f\"Selected {len(sampled_titles)} article titles for processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbTskCOa5Cy_"
   },
   "source": [
    "**How It Works**\n",
    "\n",
    "  1. **titles_from_category Function**: This function takes a Wikipedia category and retrieves all article titles within that category and its subcategories up to a specified depth.\n",
    "\n",
    "  2. **max_depth Parameter:** Controls how deep the function will go into subcategories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y78UsWuzACca"
   },
   "source": [
    "### **Chunk Documents**\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "\n",
    "* Discard less relevant-looking sections like External Links and Footnotes\n",
    "* Clean up the text by removing reference tags (e.g., ), whitespace, and super short sections\n",
    "* Split each article into sections\n",
    "* Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "* If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along     semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQ3jbtSw61eO",
    "outputId": "33bc17d3-861c-4ddc-fd9f-6ee6fe4c375b"
   },
   "outputs": [],
   "source": [
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "def all_subsections_from_section(section, parent_titles, sections_to_ignore):\n",
    "    \"\"\"Extract subsections from a Wikipedia section.\"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The all_subsections_from_section function is designed to extract subsections from a specific section of a Wikipedia article. This function is used in the context of processing a page’s text, finding headings, and breaking the content down into smaller chunks (subsections). It helps you organize the text under each heading while ignoring certain sections you don't want to include (like references or external links).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYv5v4X82gEl",
    "outputId": "b0aa84bf-18e7-4166-98ee-7ddcb5495eb1"
   },
   "outputs": [],
   "source": [
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes a Wikipedia page title and returns all the subsections of that page, along with their corresponding parent titles. It extracts the page's text, finds all the headings, and organizes the content into a list of tuples. \n",
    "\n",
    "Each tuple contains:\n",
    "\n",
    "1. A list of parent titles (starting with the page title).\n",
    "2. The text of the subsection (excluding any sub-subsections).\n",
    "\n",
    "Function Parameters:\n",
    "\n",
    "1. title: str: The title of the Wikipedia page you want to extract subsections from (e.g., \"Python (programming language)\").\n",
    "\n",
    "2. sections_to_ignore: set[str] (default: SECTIONS_TO_IGNORE): A set of section titles that should be excluded from the results. For example, you might want to skip sections like \"References\" or \"External Links.\"\n",
    "\n",
    "3. site_name: str (default: WIKI_SITE): The name of the Wikipedia site (e.g., \"en.wikipedia.org\"). This is useful if you're accessing a specific language version of Wikipedia or a different wiki site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIGilLid2i5l"
   },
   "source": [
    "The function above splits the articles into smaller sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg9m0k0d2kM7"
   },
   "source": [
    "**Clean Up Sections**\n",
    "\n",
    "We clean the sections to remove unnecessary information, such as reference tags (<ref>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83hedxcv2jvk",
    "outputId": "45c80699-f4f1-4a87-a64d-93afa3c55003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1838 sections in 179 pages.\n"
     ]
    }
   ],
   "source": [
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 89 sections, leaving 1749 sections.\n",
      "['Speed skating at the 2022 Winter Olympics']\n",
      "{{Short description|none}}\n",
      "{{Use dmy dates|date=February 2018}}\n",
      "{{Infobox Oly...\n",
      "\n",
      "['Speed skating at the 2022 Winter Olympics', '==Qualification==']\n",
      "{{Main|Speed skating at the 2022 Winter Olympics – Qualification}}\n",
      "A total qu...\n",
      "\n",
      "['Speed skating at the 2022 Winter Olympics', '==Qualification==', '===Qualification times===']\n",
      "The following qualification times were released on July 1, 2021, and were unc...\n",
      "\n",
      "['Speed skating at the 2022 Winter Olympics', '==Competition schedule==']\n",
      "The following was the competition schedule for all speed skating events. With...\n",
      "\n",
      "['Speed skating at the 2022 Winter Olympics', '==Medal summary==', '===Medal table===']\n",
      "{{Medals table\n",
      " | caption  = \n",
      " | host  = CHN\n",
      " | flag_template = flagIOC\n",
      " | ev...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# Filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    _, text = section\n",
    "    return len(text) >= 16\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n",
    "\n",
    "# Display example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    print(ws[1][:77] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6Wasvbi26gM"
   },
   "source": [
    "**Handle Text Length (Tokens)**\n",
    "\n",
    "Embeddings work best when the text is not too long. We count the tokens (words and characters) to ensure that each section is short enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Us1DUzXQ26Bq",
    "outputId": "1cfee394-935d-4480-be08-3fd6e429563a"
   },
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-4o-mini\"  # Just to select tokenizer, does not use OpenAI models\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function counts the number of tokens (units of text) in a given string based on a specific tokenizer model (in this case, gpt-4o-mini). \n",
    "\n",
    "Here's what each part does:\n",
    "\n",
    "1. Input: A text string.\n",
    "2. Output: The number of tokens in the string.\n",
    "How: It uses a tokenizer specific to the given model (like how many words or chunks the model recognizes) to \"encode\" the text and count its tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]\n",
    "    total_tokens = num_tokens(string)\n",
    "    halfway = total_tokens // 2\n",
    "    best_diff = halfway\n",
    "    for i, _ in enumerate(chunks):\n",
    "        left = delimiter.join(chunks[: i + 1])\n",
    "        left_tokens = num_tokens(left)\n",
    "        diff = abs(halfway - left_tokens)\n",
    "        if diff >= best_diff:\n",
    "            break\n",
    "        best_diff = diff\n",
    "    left = delimiter.join(chunks[:i])\n",
    "    right = delimiter.join(chunks[i:])\n",
    "    return [left, right]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits a large string into two parts at a logical breakpoint (like a sentence or paragraph).\n",
    "\n",
    "Input:\n",
    "1. string: A large text string.\n",
    "2. delimiter: The point where we want to split the string (default is a new line \\n).\n",
    "3. Output: Two parts of the string, left and right.\n",
    "\n",
    "How: It tries to find the point where the string should be split into two halves based on token count. It looks for the closest match to half the total number of tokens, and then splits the string into two logical parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_string(string: str, model: str, max_tokens: int, print_warning: bool = True) -> str:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function shortens a string to a certain number of tokens, if necessary.\n",
    "\n",
    "Input:\n",
    "\n",
    "1. string: The text to be truncated.\n",
    "2. model: The tokenizer model to use.\n",
    "3. max_tokens: The maximum number of tokens allowed.\n",
    "4. print_warning: If the string is shortened, a warning will be printed.\n",
    "\n",
    "Output: The truncated string (cut down to the allowed number of tokens).\n",
    "How: It encodes the string into tokens, and if the number of tokens exceeds the limit, it trims the string and prints a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749 Wikipedia sections split into 2052 strings.\n"
     ]
    }
   ],
   "source": [
    "def split_strings_from_subsection(subsection: tuple[list[str], str], max_tokens: int = 1000, model: str = GPT_MODEL, max_recursion: int = 5) -> list[str]:\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    if num_tokens(string) <= max_tokens:\n",
    "        return [string]\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "        left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "        if left == \"\" or right == \"\":\n",
    "            continue\n",
    "        results = []\n",
    "        for half in [left, right]:\n",
    "            half_subsection = (titles, half)\n",
    "            half_strings = split_strings_from_subsection(\n",
    "                half_subsection,\n",
    "                max_tokens=max_tokens,\n",
    "                model=model,\n",
    "                max_recursion=max_recursion - 1,\n",
    "            )\n",
    "            results.extend(half_strings)\n",
    "        return results\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "\n",
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function breaks down a large section of text into smaller pieces that are below a token limit.\n",
    "\n",
    "Input:\n",
    "1. subsection: A tuple containing a list of titles (headers) and the main text.\n",
    "2. max_tokens: The maximum number of tokens each part can have.\n",
    "3. model: The tokenizer model.\n",
    "4. max_recursion: How many times the function can call itself to keep splitting the text if it's too big.\n",
    "\n",
    "Output: A list of smaller text strings, each below the token limit.\n",
    "\n",
    "How: It tries to break the text into smaller parts by splitting it at logical places (like paragraph breaks or sentence breaks) and keeps doing so recursively until the chunks are small enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_bTrfJk3Fto"
   },
   "source": [
    "### **Generate Embeddings**\n",
    "**Prepare Text for Embedding**\n",
    "\n",
    "After splitting the sections, we convert them into numerical values (embeddings). These embeddings help computers understand the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRUXNpud3PsF",
    "outputId": "4615aa0b-3c4f-449e-df61-128ecb5c95cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1749 Wikipedia sections split into 2052 strings.\n",
      "Processing batch 0 to 99\n",
      "Processing batch 100 to 199\n",
      "Processing batch 200 to 299\n",
      "Processing batch 300 to 399\n",
      "Processing batch 400 to 499\n",
      "Processing batch 500 to 599\n",
      "Processing batch 600 to 699\n",
      "Processing batch 700 to 799\n",
      "Processing batch 800 to 899\n",
      "Processing batch 900 to 999\n",
      "Processing batch 1000 to 1099\n",
      "Processing batch 1100 to 1199\n",
      "Processing batch 1200 to 1299\n",
      "Processing batch 1300 to 1399\n",
      "Processing batch 1400 to 1499\n",
      "Processing batch 1500 to 1599\n",
      "Processing batch 1600 to 1699\n",
      "Processing batch 1700 to 1799\n",
      "Processing batch 1800 to 1899\n",
      "Processing batch 1900 to 1999\n",
      "Processing batch 2000 to 2099\n"
     ]
    }
   ],
   "source": [
    "# Split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n",
    "\n",
    "# Fetch embeddings from Nugen API\n",
    "BATCH_SIZE = 100\n",
    "EMBEDDING_MODEL = \"nugen-flash-embed\"\n",
    "embeddings = []\n",
    "\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Processing batch {batch_start} to {batch_end-1}\")\n",
    "\n",
    "    payload = {\n",
    "        \"input\": batch,\n",
    "        \"model\": EMBEDDING_MODEL\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url_api_server, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        batch_embeddings = [e['embedding'] for e in data['data']]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred: {e}\")\n",
    "        print(f\"Response content: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXpyARru3R-u"
   },
   "source": [
    "### **Save the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-QS2qxy3cEH",
    "outputId": "da2dfe5e-b777-432a-eecc-8344fec85c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to winter_olympics_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the embeddings\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n",
    "SAVE_PATH = \"winter_olympics_2022.csv\"\n",
    "df.to_csv(SAVE_PATH, index=False)\n",
    "print(f\"Embeddings saved to {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
