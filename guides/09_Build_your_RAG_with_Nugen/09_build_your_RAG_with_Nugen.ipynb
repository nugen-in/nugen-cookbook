{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRTPCWWrpu2p"
   },
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention! To learn more, visit [Nugen](https://docs.nugen.in/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAA-Q9eJg92j"
   },
   "source": [
    "## **Chat-with-PDF using Nugen APIs**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gBJgQLShhMd"
   },
   "source": [
    "In this lesson, you will learn how to chat with a PDF using Nugen's embeddings and text completion endpoints. We would go through the following:\n",
    "1. Parse the Pdf documents and create chunks \n",
    "2. Create chunk embeddings and index it in vector database\n",
    "3. Answer user queries based on contextual search from these embeddings. \n",
    "\n",
    "In order to store the embeddings, we will use Qdrant vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5DjVs0Fh5qt"
   },
   "source": [
    "### Setup and Configuration\n",
    "Importing Libraries and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E88t43DJIPEu",
    "outputId": "0a2a4e97-c44a-4609-91f4-c5049cd7d513"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet pymupdf==1.24.13 qdrant-client==1.9.1\n",
    "\n",
    "import hashlib\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKcdMb5iiESI"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* **request** : For making API calls to Nugen's language model and embedding API.\n",
    "* **fitz (PyMuPDF):** A library for reading and extracting text from PDF files.\n",
    "\n",
    "* **QdrantClient:** A client to connect and interact with the Qdrant vector database, where embeddings are stored.\n",
    "* **dotenv:** Loads environment variables from a .env file to securely manage API keys and database URLs.\n",
    "\n",
    "* **chainlit:** Used to interact with users and manage messages within a chat-like interface.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpCXACJXjVd7"
   },
   "source": [
    "### **Defining Global Variables and Model Configuration**\n",
    "\n",
    "To get started with Nugen APIs, you'll need your Nugen API key, which you can obtain for free. To access free API keys, you can visit [Nugen Dashboard](https://nugen-platform-frontend.azurewebsites.net/dashboard) Nugen offers free access to its powerful AI models, allowing you to integrate features like embeddings and language model completions at no cost.\n",
    "\n",
    "By leveraging this API key, you can seamlessly integrate Nugen’s cutting-edge APIs into your applications and start building advanced AI-powered solutions right away!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOq_-edWjnuz"
   },
   "outputs": [],
   "source": [
    "\n",
    "NUGEN_API_KEY = <enter your api key> # Replace with your actual API key from Nugen\n",
    "LLM_API_URL = \"https://api.nugen.in/inference\"\n",
    "model_llm = \"nugen-flash-instruct\"\n",
    "model_embed = \"nugen-flash-embed\"\n",
    "EMBED_DIMENSION = 768\n",
    "EMBED_CHUNK_SIZE = int(EMBED_DIMENSION * 0.95)\n",
    "EMBED_CHUNK_OVERLAP = int(EMBED_CHUNK_SIZE * 0.10)\n",
    "LLM_API_PROVIDER_KEY = NUGEN_API_KEY\n",
    "\n",
    "# Setup local Qdrant database\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "collection_name = \"pdf_embeddings\"\n",
    "top_k = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs_UNbjkjpG9"
   },
   "source": [
    "### **USE API PROVIDER:**\n",
    "\n",
    "This variable determines which provider's API will be used. In this case, it is set to \"NUGEN\", so all API calls are directed to Nugen’s services.\n",
    "\n",
    "### Nugen API Configuration:\n",
    "\n",
    "\n",
    "*   **NUGEN_API_KEY:** API key for **Nugen's domain-aligned model services**.\n",
    "*   **LLM_API_URL:** The base url for Nugen’s large language model inference API.\n",
    "*   **model_llm and model_embed:** These specify which models to use for generating instruction-based completion and text embeddings, respectively.\n",
    "      \n",
    "        1. model_llm: nugen-flash-instruct (used for answering user queries).\n",
    "        2. model_embed: nugen-flash-embed (used for generating embeddings from text).\n",
    "    \n",
    "### Embedding Parameters:\n",
    "\n",
    "*   **EMBED_DIMENSION:** Dimension of the embedding vector (768 for Nugen's embeddings).\n",
    "\n",
    "*  **EMBED_CHUNK_SIZE and EMBED_CHUNK_OVERLAP:** This is a parameter to create chunks the contents of the PDF, which is then used to create chunk embeddings. A chunk is the amount of text processed together, and overlap ensures continuity between adjacent chunks.\n",
    "\n",
    "**QdrantClient:** The client object for connecting to Qdrant (the vector database where embeddings are stored). We use a local instantiation of the qdrant client. Note: If the jupyter notebook kernel is closed, you would need to re-index the embeddings. This is not recommended for working with larger documents. We strongly recommend following the documentation of Qdrant for local deployment here: https://qdrant.tech/documentation/quickstart/. To enable local persistance of the data. For production use cases, please refer to the official documentation here: https://qdrant.tech/documentation/guides/installation/\n",
    "\n",
    "**Collection Name:** This is the name of the Qdrant collection where embeddings related to the PDFs will be stored.\n",
    "\n",
    "**top_k:** Defines the number of top results to retrieve from the Qdrant database when searching for relevant context based on the user query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALDirP5xm7C0"
   },
   "source": [
    "### **Setting up the Qdrant Collection**\n",
    "\n",
    "Now, let’s set up the Qdrant collection where the PDF embeddings will be stored. This function checks if the collection already exists, and if not, it creates a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctWJ9Cbnm86m"
   },
   "outputs": [],
   "source": [
    "def setup_qdrant_collection(qdrant_client, collection_name, embed_dim):\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections().collections\n",
    "        if collection_name not in [collection.name for collection in collections]:\n",
    "            qdrant_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(size=embed_dim, distance=\"Cosine\")\n",
    "            )\n",
    "            print(f\"Collection '{collection_name}' created.\")\n",
    "        else:\n",
    "            print(f\"Collection '{collection_name}' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Qdrant collection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaF13LcTnBWa"
   },
   "source": [
    "* This function checks if a collection (i.e., a \"bucket\" for storing embeddings) already exists in Qdrant.\n",
    "\n",
    "* If the collection does not exist, it creates a new one with vector size (embed_dim) based on the embedding dimensions of the Nugen model.\n",
    "\n",
    "* Cosine distance is used as the metric for comparing vectors, which is standard for similarity searches.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLp3FAeWnYky"
   },
   "source": [
    "### **Extracting Text and Splitting PDF into Chunks**\n",
    "\n",
    "The next step is to extract text from a PDF file and split it into manageable chunks. These chunks will then be converted into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FThcKkDDnc_2"
   },
   "outputs": [],
   "source": [
    "# 2. Convert PDF to text chunks\n",
    "def pdf_to_text_chunks(pdf_path, chunk_size, overlap_size):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\".join([page.get_text() for page in doc])\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap_size)]\n",
    "    print(f\"Extracted Text: {text[:500]}...\")  # Print a sample of extracted text\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "    return text, chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otR4YGYxnhgZ"
   },
   "source": [
    "* This function opens the PDF using PyMuPDF (fitz) and extracts all the text from each page of the document.\n",
    "* Read more about pymupdf [here](https://pymupdf.readthedocs.io/)\n",
    "* The entire text is then split into chunks of a specific size (chunk_size) with some overlap (overlap_size). Overlapping chunks help maintain continuity in embeddings, which can improve retrieval performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQHTlXI0np_6"
   },
   "source": [
    "### **Generating Embeddings for Text Chunks**\n",
    "\n",
    "Now that we have text chunks, we need to generate embeddings for them using the Nugen API. This function takes each chunk and sends it to the Nugen API to generate an embedding.\n",
    "\n",
    "To read more about Nugen API and access free API keys, you can visit [Nugen Intelligence](https://docs.nugen.in/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DCJaWpOnv_y"
   },
   "outputs": [],
   "source": [
    "# 3. Create embedding using Nugen API\n",
    "def create_embedding(text, model_embed):\n",
    "    url = f\"{LLM_API_URL}/embeddings\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_PROVIDER_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model_embed,\n",
    "        \"input\": text\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        embedding = response.json()['data'][0]['embedding']\n",
    "        print(f\"Embedding generated for text (length {len(text)}): {embedding[:5]}...\")\n",
    "        return embedding\n",
    "    else:\n",
    "        print(f\"Error in embedding API: {response.status_code}, {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70pRPutUnyA5"
   },
   "source": [
    "* This function calls the Nugen embedding API to generate embeddings for the given text.\n",
    "* It sends a POST request to Nugen’s /embeddings endpoint with the text data and embedding model (model_embed).\n",
    "* The function returns the vector embedding of the text, which is later stored in Qdrant DB.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upcM9hk3oHz4"
   },
   "source": [
    "### **Storing Embeddings in Qdrant DB**\n",
    "\n",
    "Once embeddings are generated, we need to store them in Qdrant DB. This function handles the creation of points and uploads them to the Qdrant collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0ma4MISoJzK"
   },
   "outputs": [],
   "source": [
    "# 4. Store embeddings in Qdrant\n",
    "def store_embeddings(chunks, file_path, file_hash, user_id, thread_id, message_id, collection_name):\n",
    "    try:\n",
    "        points = []\n",
    "        for id, chunk in enumerate(chunks):\n",
    "            embedding = create_embedding(chunk, model_embed)\n",
    "            if embedding:  # Proceed only if embedding was successfully generated\n",
    "                points.append(PointStruct(\n",
    "                    id=id, \n",
    "                    vector=embedding, \n",
    "                    payload={\n",
    "                        \"chunk_text\": chunk,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_hash\": file_hash,\n",
    "                        \"user_id\": user_id,\n",
    "                        \"thread_id\": thread_id,\n",
    "                        \"message_id\": message_id\n",
    "                    }\n",
    "                ))\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "        print(f\"Embeddings stored successfully. Total points: {len(points)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing embeddings: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDnD7wb1oMnZ"
   },
   "source": [
    "The above function chunks the input text and stores each chunk's embedding in Qdrant DB.\n",
    "\n",
    "* Key Parameters:\n",
    "  * chunks: List of text chunks to be embedded\n",
    "  * file_path: Path of the file being processed\n",
    "  * file_hash: Hash of the file to uniquely identify it\n",
    "  * collection_name: Qdrant collection to store the embeddings\n",
    "\n",
    "* Return: No return value, but embeddings are upserted (inserted or updated) into Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjHLFewCoMgo"
   },
   "source": [
    "### **Retrieving Relevant Context from Qdrant DB**\n",
    "\n",
    "Once the embeddings are stored, we can query Qdrant DB to retrieve the most relevant chunks based on a user's query. This function generates an embedding for the query and searches for similar embeddings in Qdrant DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc8AmM9lorub"
   },
   "outputs": [],
   "source": [
    "# 5. Retrieve relevant chunks based on query\n",
    "from qdrant_client.models import Filter, FieldCondition, models\n",
    "\n",
    "def simple_rag_retrieve(query, top_k, user_id, thread_id, collection_name):\n",
    "    try:\n",
    "        query_embedding = create_embedding(query, model_embed)\n",
    "\n",
    "        # Apply the filter with user_id and thread_id to the query\n",
    "        user_query_filter = Filter(\n",
    "            must=[\n",
    "                FieldCondition(key=\"user_id\", match=models.MatchValue(value=user_id)),\n",
    "                FieldCondition(key=\"thread_id\", match=models.MatchValue(value=thread_id))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        search_result = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=top_k,\n",
    "            query_filter=user_query_filter\n",
    "        )\n",
    "\n",
    "        retrieved_texts = [hit.payload['chunk_text'] for hit in search_result]\n",
    "        print(f\"Search Results: {retrieved_texts[:2]}...\")  # Show a sample of results\n",
    "        return \"\\n\".join(retrieved_texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving context: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItMu2v0RotvH"
   },
   "source": [
    "**Search**: We use the query embedding to search for the most relevant chunks in the Qdrant collection.\n",
    "\n",
    "**Filters**: The filter ensures that the results are specific to a particular user and thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KspIXQWQpNPH"
   },
   "source": [
    "### **Generate Response Using Nugen API**\n",
    "\n",
    "After retrieving the relevant text chunks, we can use the Nugen API to generate a response based on the context and user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S5JoBqVpP58"
   },
   "outputs": [],
   "source": [
    "# 6. Generate response using Nugen API\n",
    "def generate_llm_response(context, query, model_llm):\n",
    "    url = f\"{LLM_API_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_PROVIDER_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model_llm,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Answer the question: {query}\"}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        response_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(f\"LLM Response: {response_text}\")\n",
    "        return response_text\n",
    "    else:\n",
    "        print(f\"Error in LLM API: {response.status_code}, {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru0B2mG4pVPT"
   },
   "source": [
    "* This function sends a POST request to the Nugen API to generate a response based on the retrieved context and the user query.\n",
    "\n",
    "* **messages:** The request includes the context retrieved from Qdrant and the user’s query. The assistant uses these messages to generate a relevant response.\n",
    "\n",
    "* The Nugen API processes this request and returns a completion (answer) that is sent back to the user.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. End-to-end PDF embedding and querying\n",
    "def embed_pdf_and_query(file_path, user_query, user_id, thread_id, message_id, collection_name):\n",
    "    # Embed the PDF\n",
    "    print(f\"Processing PDF: {file_path}\")\n",
    "    _, chunks = pdf_to_text_chunks(file_path, EMBED_CHUNK_SIZE, EMBED_CHUNK_OVERLAP)\n",
    "    file_hash = hashlib.md5(open(file_path, 'rb').read()).hexdigest()\n",
    "    store_embeddings(chunks, file_path, file_hash, user_id, thread_id, message_id, collection_name)\n",
    "    \n",
    "    # Retrieve relevant context for the query\n",
    "    context = simple_rag_retrieve(user_query, top_k, user_id, thread_id, collection_name)\n",
    "    \n",
    "    # Generate response using the context and the query\n",
    "    if context:\n",
    "        return generate_llm_response(context, user_query, model_llm)\n",
    "    else:\n",
    "        print(\"No relevant context found.\")\n",
    "        return None\n",
    "\n",
    "# Example of usage\n",
    "file_path = \"registration_act_1908.pdf\"  # Replace with actual PDF file path\n",
    "user_query = \"What are the main conclusions of this document?\"\n",
    "user_id = \"user123\"\n",
    "thread_id = \"thread456\"\n",
    "message_id = \"msg789\"\n",
    "\n",
    "setup_qdrant_collection(qdrant_client, collection_name, EMBED_DIMENSION)\n",
    "response = embed_pdf_and_query(file_path, user_query, user_id, thread_id, message_id, collection_name)\n",
    "\n",
    "if response:\n",
    "    print(f\"Final Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Flow**\n",
    "\n",
    "\n",
    "Finally, we can put everything together into an end-to-end flow that processes a PDF, stores the embeddings, retrieves relevant context, and generates an answer for the user’s query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTS2Hjf7pnno"
   },
   "source": [
    "By following this structure, the model enables users to upload PDFs, extract meaningful information from them, and ask questions that are answered based on the embedded content in the document. All of this is powered by Nugen’s APIs and the Qdrant vector database for high-quality search and retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "By following this cookbook, you now have a complete solution for processing PDF documents, generating embeddings with the Nugen API, storing them in Qdrant, and retrieving relevant information based on user queries. This solution can be expanded or modified for different use cases such as knowledge bases, document search, or other information retrieval applications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
