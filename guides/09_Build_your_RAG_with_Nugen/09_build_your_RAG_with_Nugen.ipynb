{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRTPCWWrpu2p"
   },
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention! To learn more, visit [Nugen](https://docs.nugen.in/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAA-Q9eJg92j"
   },
   "source": [
    "## **Chat-with-PDF using Nugen APIs**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gBJgQLShhMd"
   },
   "source": [
    "In this lesson, you will learn how to chat with a PDF using Nugen's embeddings and text completion endpoints. We would go through the following:\n",
    "1. Parse the Pdf documents and create chunks \n",
    "2. Create chunk embeddings and index it in vector database\n",
    "3. Answer user queries based on contextual search from these embeddings. \n",
    "\n",
    "In order to store the embeddings, we will use Qdrant vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Create a .env file with (bash):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUGEN_API_KEY=\"write your api key here\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5DjVs0Fh5qt"
   },
   "source": [
    "### Setup and Configuration\n",
    "Importing Libraries and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E88t43DJIPEu",
    "outputId": "0a2a4e97-c44a-4609-91f4-c5049cd7d513"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Filter, FieldCondition, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKcdMb5iiESI"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* **request** : For making API calls to Nugen's language model and embedding API.\n",
    "* **fitz (PyMuPDF):** A library for reading and extracting text from PDF files.\n",
    "\n",
    "* **QdrantClient:** A client to connect and interact with the Qdrant vector database, where embeddings are stored.\n",
    "* **dotenv:** Loads environment variables from a .env file to securely manage API keys and database URLs.\n",
    "\n",
    "* **chainlit:** Used to interact with users and manage messages within a chat-like interface.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Environment Variable Securely\n",
    " It prevent the API key Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "NUGEN_API_KEY = os.getenv(\"NUGEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Setup\n",
    "This snippet configures logging to record messages to a file named audit_log.txt with timestamps, severity levels, and detailed messages, enabling efficient audit tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"audit_log.txt\", level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpCXACJXjVd7"
   },
   "source": [
    "### **Defining Global Variables and Model Configuration**\n",
    "\n",
    "To get started with Nugen APIs, you'll need your Nugen API key, which you can obtain for free. To access free API keys, you can visit [Nugen Dashboard](https://nugen-platform-frontend.azurewebsites.net/dashboard) Nugen offers free access to its powerful AI models, allowing you to integrate features like embeddings and language model completions at no cost.\n",
    "\n",
    "By leveraging this API key, you can seamlessly integrate Nugen’s cutting-edge APIs into your applications and start building advanced AI-powered solutions right away!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOq_-edWjnuz"
   },
   "outputs": [],
   "source": [
    "\n",
    "LLM_API_URL = \"https://api.nugen.cloud/inference\"\n",
    "model_llm = \"nugen-flash-instruct\"\n",
    "model_embed = \"nugen-flash-embed\"\n",
    "EMBED_DIMENSION = 768\n",
    "EMBED_CHUNK_SIZE = int(EMBED_DIMENSION * 0.95)\n",
    "EMBED_CHUNK_OVERLAP = int(EMBED_CHUNK_SIZE * 0.10)\n",
    "LLM_API_PROVIDER_KEY = NUGEN_API_KEY\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs_UNbjkjpG9"
   },
   "source": [
    "### **USE API PROVIDER:**\n",
    "\n",
    "This variable determines which provider's API will be used. In this case, it is set to \"NUGEN\", so all API calls are directed to Nugen’s services.\n",
    "\n",
    "### Nugen API Configuration:\n",
    "\n",
    "\n",
    "*   **NUGEN_API_KEY:** API key for **Nugen's domain-aligned model services**.\n",
    "*   **LLM_API_URL:** The base url for Nugen’s large language model inference API.\n",
    "*   **model_llm and model_embed:** These specify which models to use for generating instruction-based completion and text embeddings, respectively.\n",
    "      \n",
    "        1. model_llm: nugen-flash-instruct (used for answering user queries).\n",
    "        2. model_embed: nugen-flash-embed (used for generating embeddings from text).\n",
    "    \n",
    "### Embedding Parameters:\n",
    "\n",
    "*   **EMBED_DIMENSION:** Dimension of the embedding vector (768 for Nugen's embeddings).\n",
    "\n",
    "*  **EMBED_CHUNK_SIZE and EMBED_CHUNK_OVERLAP:** This is a parameter to create chunks the contents of the PDF, which is then used to create chunk embeddings. A chunk is the amount of text processed together, and overlap ensures continuity between adjacent chunks.\n",
    "\n",
    "**QdrantClient:** The client object for connecting to Qdrant (the vector database where embeddings are stored). We use a local instantiation of the qdrant client. Note: If the jupyter notebook kernel is closed, you would need to re-index the embeddings. This is not recommended for working with larger documents. We strongly recommend following the documentation of Qdrant for local deployment here: https://qdrant.tech/documentation/quickstart/. To enable local persistance of the data. For production use cases, please refer to the official documentation here: https://qdrant.tech/documentation/guides/installation/\n",
    "\n",
    "**Collection Name:** This is the name of the Qdrant collection where embeddings related to the PDFs will be stored.\n",
    "\n",
    "**top_k:** Defines the number of top results to retrieve from the Qdrant database when searching for relevant context based on the user query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALDirP5xm7C0"
   },
   "source": [
    "### **Setting up the Qdrant Collection**\n",
    "\n",
    "Now, let’s set up the Qdrant collection where the PDF embeddings will be stored. This function checks if the collection already exists, and if not, it creates a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "collection_name = \"pdf_embeddings\"\n",
    "top_k = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctWJ9Cbnm86m"
   },
   "outputs": [],
   "source": [
    "def setup_qdrant_collection(qdrant_client, collection_name, embed_dim):\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections().collections\n",
    "        if collection_name not in [collection.name for collection in collections]:\n",
    "            qdrant_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(size=embed_dim, distance=\"Cosine\")\n",
    "            )\n",
    "            print(f\"Collection '{collection_name}' created.\")\n",
    "        else:\n",
    "            print(f\"Collection '{collection_name}' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Qdrant collection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaF13LcTnBWa"
   },
   "source": [
    "* This function checks if a collection (i.e., a \"bucket\" for storing embeddings) already exists in Qdrant.\n",
    "\n",
    "* If the collection does not exist, it creates a new one with vector size (embed_dim) based on the embedding dimensions of the Nugen model.\n",
    "\n",
    "* Cosine distance is used as the metric for comparing vectors, which is standard for similarity searches.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLp3FAeWnYky"
   },
   "source": [
    "### **Extracting Text and Splitting PDF into Chunks**\n",
    "\n",
    "The next step is to extract text from a PDF file and split it into manageable chunks. These chunks will then be converted into embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function implements paragraph-aware semantic chunking for PDFs. It extracts text while preserving paragraph structure, splits it into manageable chunks with a specified size, and ensures logical text segmentation for better processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FThcKkDDnc_2"
   },
   "outputs": [],
   "source": [
    "# Paragraph-aware semantic chunking (Improved Chunking Strategy)\n",
    "def pdf_to_text_chunks(pdf_path, chunk_size, overlap_size):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    paragraphs = []\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "        paragraphs.extend([p.strip() for p in text.split('\\n') if p.strip()])\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) <= chunk_size:\n",
    "            current_chunk += para + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = para + \" \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return \"\\n\".join(paragraphs), chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otR4YGYxnhgZ"
   },
   "source": [
    "* This function opens the PDF using PyMuPDF (fitz) and extracts all the text from each page of the document.\n",
    "* Read more about pymupdf [here](https://pymupdf.readthedocs.io/)\n",
    "* The entire text is then split into chunks of a specific size (chunk_size) with some overlap (overlap_size). Overlapping chunks help maintain continuity in embeddings, which can improve retrieval performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitization (Prevents Prompt Injection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function removes all characters from the input text except letters, numbers, spaces, punctuation, and parentheses, ensuring clean and safe input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize_input(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9 .,?()\\n]\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQHTlXI0np_6"
   },
   "source": [
    "### **Generating Embeddings for Text Chunks**\n",
    "\n",
    "Now that we have text chunks, we need to generate embeddings for them using the Nugen API. This function takes each chunk and sends it to the Nugen API to generate an embedding.\n",
    "\n",
    "To read more about Nugen API and access free API keys, you can visit [Nugen Intelligence](https://docs.nugen.in/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates embeddings with a retry mechanism and exponential backoff for resilience. If the API fails after three attempts, it safely returns a mock embedding to maintain functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DCJaWpOnv_y"
   },
   "outputs": [],
   "source": [
    "# Embedding with retry + fallback (Security & Monitoring)\n",
    "def create_embedding(text, model_embed):\n",
    "    url = f\"{LLM_API_URL}/embeddings\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_PROVIDER_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\"model\": model_embed, \"input\": text}\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = requests.post(url, json=data, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                embedding = response.json()['data'][0]['embedding']\n",
    "                logging.info(f\"Embedding token usage: {len(text.split())}\")\n",
    "                return embedding\n",
    "        except Exception as e:\n",
    "            time.sleep(2 ** attempt)\n",
    "    print(\" API not reachable. Returning mock embedding.\")\n",
    "    return [random.random() for _ in range(EMBED_DIMENSION)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70pRPutUnyA5"
   },
   "source": [
    "* This function calls the Nugen embedding API to generate embeddings for the given text.\n",
    "* It sends a POST request to Nugen’s /embeddings endpoint with the text data and embedding model (model_embed).\n",
    "* The function returns the vector embedding of the text, which is later stored in Qdrant DB.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upcM9hk3oHz4"
   },
   "source": [
    "### **Storing Embeddings in Qdrant DB**\n",
    "\n",
    "Once embeddings are generated, we need to store them in Qdrant DB. This function handles the creation of points and uploads them to the Qdrant collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function stores vector embeddings in Qdrant by associating them with metadata, enabling efficient retrieval. It ensures embeddings are paired with meaningful context for seamless query handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0ma4MISoJzK"
   },
   "outputs": [],
   "source": [
    "# Store vector embeddings in Qdrant with metadata (Metadata + Retrieval)\n",
    "def store_embeddings(chunks, file_path, file_hash, user_id, thread_id, message_id, collection_name):\n",
    "    points = []\n",
    "    for id, chunk in enumerate(chunks):\n",
    "        embedding = create_embedding(chunk, model_embed)\n",
    "        if embedding:\n",
    "            points.append(PointStruct(\n",
    "                id=id,\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"chunk_text\": chunk,\n",
    "                    \"file_path\": file_path,\n",
    "                    \"file_hash\": file_hash,\n",
    "                    \"user_id\": user_id,\n",
    "                    \"thread_id\": thread_id,\n",
    "                    \"message_id\": message_id,\n",
    "                    \"chunk_id\": id\n",
    "                }\n",
    "            ))\n",
    "    qdrant_client.upsert(collection_name=collection_name, points=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDnD7wb1oMnZ"
   },
   "source": [
    "The above function chunks the input text and stores each chunk's embedding in Qdrant DB.\n",
    "\n",
    "* Key Parameters:\n",
    "  * chunks: List of text chunks to be embedded\n",
    "  * file_path: Path of the file being processed\n",
    "  * file_hash: Hash of the file to uniquely identify it\n",
    "  * collection_name: Qdrant collection to store the embeddings\n",
    "\n",
    "* Return: No return value, but embeddings are upserted (inserted or updated) into Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjHLFewCoMgo"
   },
   "source": [
    "### **Retrieving Relevant Context from Qdrant DB**\n",
    "\n",
    "Once the embeddings are stored, we can query Qdrant DB to retrieve the most relevant chunks based on a user's query. This function generates an embedding for the query and searches for similar embeddings in Qdrant DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc8AmM9lorub"
   },
   "outputs": [],
   "source": [
    "# 5. Retrieve relevant chunks based on query\n",
    "from qdrant_client.models import Filter, FieldCondition, models\n",
    "\n",
    "def simple_rag_retrieve(query, top_k, user_id, thread_id, collection_name):\n",
    "    try:\n",
    "        query_embedding = create_embedding(query, model_embed)\n",
    "\n",
    "        # Apply the filter with user_id and thread_id to the query\n",
    "        user_query_filter = Filter(\n",
    "            must=[\n",
    "                FieldCondition(key=\"user_id\", match=models.MatchValue(value=user_id)),\n",
    "                FieldCondition(key=\"thread_id\", match=models.MatchValue(value=thread_id))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        search_result = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=top_k,\n",
    "            query_filter=user_query_filter\n",
    "        )\n",
    "\n",
    "        retrieved_texts = [hit.payload['chunk_text'] for hit in search_result]\n",
    "        print(f\"Search Results: {retrieved_texts[:2]}...\")  # Show a sample of results\n",
    "        return \"\\n\".join(retrieved_texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving context: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItMu2v0RotvH"
   },
   "source": [
    "**Search**: We use the query embedding to search for the most relevant chunks in the Qdrant collection.\n",
    "\n",
    "**Filters**: The filter ensures that the results are specific to a particular user and thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KspIXQWQpNPH"
   },
   "source": [
    "### **Generate Response Using Nugen API**\n",
    "\n",
    "After retrieving the relevant text chunks, we can use the Nugen API to generate a response based on the context and user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates responses from an LLM by providing query-specific context, utilizing structured prompts for better relevance and accuracy. It also logs token usage and the generated response for monitoring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S5JoBqVpP58"
   },
   "outputs": [],
   "source": [
    "# Generate LLM response with context Using NUGEN API(Improved Prompt Design)\n",
    "def generate_llm_response(context, query, model_llm):\n",
    "    url = f\"{LLM_API_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_PROVIDER_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model_llm,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Answer the question: {query}\"}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        logging.info(f\"LLM usage: {result.get('usage', {}).get('total_tokens', 0)}\")\n",
    "        logging.info(f\"Response: {response_text}\")\n",
    "        return response_text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru0B2mG4pVPT"
   },
   "source": [
    "* This function sends a POST request to the Nugen API to generate a response based on the retrieved context and the user query.\n",
    "\n",
    "* **messages:** The request includes the context retrieved from Qdrant and the user’s query. The assistant uses these messages to generate a relevant response.\n",
    "\n",
    "* The Nugen API processes this request and returns a completion (answer) that is sent back to the user.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes a PDF document, embeds its content with metadata into a vector database, retrieves relevant context for a given query, and generates a response using an LLM. It ensures an efficient end-to-end workflow for document querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. End-to-end PDF embedding and querying\n",
    "def embed_pdf_and_query(file_path, user_query, user_id, thread_id, message_id, collection_name):\n",
    "    print(f\"Processing PDF: {file_path}\")\n",
    "    _, chunks = pdf_to_text_chunks(file_path, EMBED_CHUNK_SIZE, EMBED_CHUNK_OVERLAP)\n",
    "    file_hash = hashlib.sha256(open(file_path, 'rb').read()).hexdigest()\n",
    "    store_embeddings(chunks, file_path, file_hash, user_id, thread_id, message_id, collection_name)\n",
    "    context = simple_rag_retrieve(user_query, top_k, user_id, thread_id, collection_name)\n",
    "    if context:\n",
    "        response = generate_llm_response(context, user_query, model_llm)\n",
    "        logging.info(f\"User feedback requested for: {user_query}\")\n",
    "        return response\n",
    "    else:\n",
    "        print(\"No relevant context found.\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Validate PDF (File integrity Check)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function downloads a PDF from the given URL, saves it to the specified file path, computes its SHA256 hash for integrity verification, and returns both the file path and hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_with_hash(pdf_url, file_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    file_hash = hashlib.sha256(open(file_path, 'rb').read()).hexdigest()\n",
    "    print(\"PDF SHA256 Hash:\", file_hash)\n",
    "    return file_path, file_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Sample\n",
    "This snippet initializes a Qdrant collection, downloads a PDF with its hash, embeds the PDF content, queries for relevant information, and prints the final answer based on the document's conclusions. It integrates the end-to-end workflow seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_qdrant_collection(qdrant_client, collection_name, EMBED_DIMENSION)\n",
    "pdf_url = \"https://www.indiacode.nic.in/bitstream/123456789/13236/1/the_registration_act,_1908.pdf\"\n",
    "file_path, _ = download_pdf_with_hash(pdf_url, \"registration_act_1908.pdf\")\n",
    "response = embed_pdf_and_query(\n",
    "    file_path=file_path,\n",
    "    user_query=\"What are the main conclusions of this document?\",\n",
    "    user_id=\"user123\",\n",
    "    thread_id=\"thread456\",\n",
    "    message_id=\"msg789\",\n",
    "    collection_name=collection_name\n",
    ")\n",
    "if response:\n",
    "    print(\"Final Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Flow**\n",
    "\n",
    "\n",
    "Finally, we can put everything together into an end-to-end flow that processes a PDF, stores the embeddings, retrieves relevant context, and generates an answer for the user’s query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTS2Hjf7pnno"
   },
   "source": [
    "By following this structure, the model enables users to upload PDFs, extract meaningful information from them, and ask questions that are answered based on the embedded content in the document. All of this is powered by Nugen’s APIs and the Qdrant vector database for high-quality search and retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "By following this cookbook, you now have a complete solution for processing PDF documents, generating embeddings with the Nugen API, storing them in Qdrant, and retrieving relevant information based on user queries. This solution can be expanded or modified for different use cases such as knowledge bases, document search, or other information retrieval applications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
