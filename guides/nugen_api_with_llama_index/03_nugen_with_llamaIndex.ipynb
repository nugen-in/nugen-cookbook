{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nugen APIs with LlamaIndex** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Introduction: What are Completion Models?**\n",
    "\n",
    "Completion models are AI models that generate text based on an input prompt. These models are designed to \"complete\" a given sentence, idea, or any form of text. For example, if you provide the prompt \"The sky is\", the completion model might respond with \"blue and vast on a sunny day.\" Completion models are widely used in chatbots, automated content generation, coding assistants, and much more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cookbook, we’ll explore how to use **Nugen's** powerful completion models via their API and how we can integrate them with the **LlamaIndex** framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Getting Started with Nugen Completion Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nugen provides powerful AI models through its API for various tasks, including text completion. To get started with Nugen, follow these steps:\n",
    "\n",
    "**Step 1: Get Access to Nugen APIs**\n",
    "\n",
    "To use Nugen's models, you need an API key. You can access Nugen API key from [here](https://docs.nugen.in/) for FREE! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Install required libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\parimal\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\transformers\\\\models\\\\deprecated\\\\trajectory_transformer\\\\convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip --quiet install llama-index requests llama-index-llms-openllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Example of Nugen Completion Model API Call**\n",
    "\n",
    "Now that you have your API key, let's make an API request to Nugen's completion endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Nugen API endpoint for text completions\n",
    "url = \"https://api.nugen.in/inference/completions\"\n",
    "\n",
    "# Prepare the request payload\n",
    "payload = {\n",
    "    \"model\": \"nugen-flash-instruct\",  # The model to use\n",
    "    \"prompt\": \"The sky is\",  # Prompt to generate text for\n",
    "    \"max_tokens\": 400,  # Limit for the number of tokens in the output\n",
    "    \"temperature\": 1  # Controls creativity in the output\n",
    "}\n",
    "\n",
    "api_key = <get-your-api-key>\n",
    "\n",
    "# Include your API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer{api_key}\",  # Replace with your actual API key\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Send the request to Nugen API\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# Print the response from Nugen API\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details of the API Parameters:**\n",
    "\n",
    "* Model: Specifies which model you are using, such as \"nugen-flash-instruct\".\n",
    "* Prompt: The input text you want to complete.\n",
    "* Max Tokens: The maximum number of tokens (words or symbols) the model should generate.\n",
    "* Temperature: Affects the randomness of the model's output. A lower value like 0.1 will make the output more deterministic, while a higher value like 1.0 will allow for more creativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What is LlamaIndex? Using Nugen with LlamaIndex**\n",
    "\n",
    "LlamaIndex (formerly known as GPT Index) is a framework that provides easy access to language models like GPT, and it allows you to integrate various LLMs into your applications with minimal setup.\n",
    "\n",
    "**How to Use Nugen with LlamaIndex**\n",
    "\n",
    "To integrate Nugen’s API with LlamaIndex, you can use the OpenLLM class, which acts as an interface to connect LlamaIndex with different language models, including Nugen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Configure LlamaIndex with Nugen API:**\n",
    "\n",
    "Here's a simple example of how to configure LlamaIndex to use Nugen’s completion model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from llama_index.llms.openllm import OpenLLM\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# Set up the Nugen model with LlamaIndex\n",
    "llm = OpenLLM(\n",
    "    model=\"nugen-flash-instruct\",  # Nugen model name\n",
    "    api_base=\"https://api.nugen.in/inference/\",  # Nugen's API base URL\n",
    "    api_key=api_key,  # Your API key\n",
    "    max_tokens=1000  # Max token limit for completion\n",
    ")\n",
    "\n",
    "# Test the model with a sample prompt\n",
    "completion_response = llm.complete(\"To infinity, and\")\n",
    "print(completion_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Parameters:**\n",
    "\n",
    "* model: The Nugen model to use (nugen-flash-instruct).\n",
    "* api_base: Base URL for the Nugen API.\n",
    "* api_key: Your Nugen API key for authentication.\n",
    "* max_tokens: Defines how much text the model should generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Code:**\n",
    "\n",
    "Once set up, you can interact with the Nugen model through LlamaIndex, providing prompts and receiving text completions just like in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "In this guide, we explored how to use Nugen’s completion models both directly through API calls and by integrating them with the LlamaIndex framework. Nugen’s models provide high-quality text generation capabilities, which, when paired with LlamaIndex, can streamline interactions with LLMs for various tasks.\n",
    "\n",
    "By following this cookbook, junior and beginner developers should now be able to:\n",
    "\n",
    "* Understand the basics of completion models.\n",
    "* Get started with Nugen APIs, including setting up API keys and making requests.\n",
    "* Use LlamaIndex to integrate Nugen models seamlessly into their applications.\n",
    "* You are now equipped to build your own applications using Nugen and LlamaIndex!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
