{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Integrating Nugen’s Completion Model with LangChain’s Embedding Model**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This guide demonstrates how to combine the strengths of Nugen’s completion API with LangChain’s embedding model to build a text-based application. You can leverage LangChain's embedding capabilities for document storage and retrieval, while using Nugen's completion model for generating natural language outputs based on those embeddings.\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "* **Nugen Completion Model**: An API that generates text completions based on input prompts.\n",
    "* **LangChain Embedding Model**: A framework used for generating text embeddings and storing them in vector stores for retrieval.\n",
    "* **Embeddings**: Numerical representations of text for semantic understanding.\n",
    "* **Vector Store**: A data structure that stores embeddings for fast retrieval.\n",
    "* **Completions**: Generated text responses based on a given input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Set Up the Environment\n",
    "\n",
    "Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet -U requests langchain  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Generate Embeddings Using OpenAIEmbeddings\n",
    "\n",
    "You first generate embeddings for your documents (such as sentences) using OpenAIEmbeddings. In this case, you're embedding a set of sentences. Here's the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the OpenAI embeddings model with the API key\n",
    "embeddings_model = OpenAIEmbeddings(api_key=\"<--openai api key-->\")\n",
    "\n",
    "# Create embeddings for your documents\n",
    "documents = [\n",
    "    \"Hi there!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"What's your name?\",\n",
    "    \"My friends call me World\",\n",
    "    \"Hello World!\"\n",
    "]\n",
    "\n",
    "# Get the embeddings for the documents\n",
    "embeddings = embeddings_model.embed_documents(documents)\n",
    "\n",
    "# Check the shape of the embeddings\n",
    "len(embeddings), len(embeddings[0])  # len(embeddings) gives the number of documents, len(embeddings[0]) gives the dimension of each embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Create an Embedded Query\n",
    "\n",
    "You can also create embeddings for a query (e.g., \"What was the name mentioned in the conversation?\") using the same model. This will help you compare the query to the embedded documents for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005325127858668566,\n",
       " -0.0006705899722874165,\n",
       " 0.038958556950092316,\n",
       " -0.002979300217702985,\n",
       " -0.008908206596970558]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an embedding for your query\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "\n",
    "# Check the first few elements of the embedded query to see how it's represented\n",
    "embedded_query[:5]  # Output the first 5 values of the query's embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Search for Relevant Documents Using Similarity Search\n",
    "\n",
    "Now, you can use the embeddings (documents and queries) to find the most relevant documents based on similarity. You would compare the embedded_query with your document embeddings.\n",
    "\n",
    "Here’s an example of how you might integrate this with the Nugen completion model for answering the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " as I am so well-traveled. \n",
      "The context of the conversation is: You were talking about traveling to other countries. \n",
      "The role-play is: You are an experienced traveler and the other person is a student interested in traveling. \n",
      "\n",
      "Other person: I've always wanted to travel but I'm not sure where to start. Do you have any advice?\n",
      "\n",
      "You: Ah, I totally understand! I was in your shoes not too long ago. Let me tell you, it's a whole different world out there. I've been to so many countries, my friends even call me World! (laughs) \n",
      "\n",
      "Anyway, to get started, I'd say research, research, research! Look into different cultures, customs, and ways of life. It's fascinating to learn about the history and traditions of a place before you visit. Plus, it'll help you navigate unfamiliar surroundings and make the most of your trip.\n",
      "\n",
      "Also, don't be afraid to take the leap and book that ticket! You can always plan and prepare, but sometimes you just need to go for it. And don't worry if you make mistakes – that's all part of the adventure, right?\n",
      "\n",
      "Lastly, consider backpacking or joining a group tour. It's a great way to meet fellow travelers and learn from their experiences. You can also find some amazing deals and discounts that way.\n",
      "\n",
      "What do you think? Are you ready to start planning your first trip?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Nugen API Key\n",
    "api_key = \"<--nugen-api-key-->\"\n",
    "\n",
    "def get_completions(prompt):\n",
    "    url = \"https://api.nugen.in/inference/completions\"\n",
    "    payload = {\n",
    "        \"max_tokens\": 400,\n",
    "        \"model\": \"nugen-flash-instruct\",\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 1\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['text']  # Extracting the completion text\n",
    "    else:\n",
    "        print(f\"Error fetching completions: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Assume you found a relevant document from your vector store or search\n",
    "relevant_document = \"My friends call me World\"\n",
    "\n",
    "# Get a completion (response) based on the relevant document\n",
    "completion_response = get_completions(f\"The name mentioned in the conversation is: {relevant_document}\")\n",
    "\n",
    "# Print the completion result\n",
    "print(completion_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How It Works:**\n",
    "\n",
    "Generate Embeddings: You create embeddings for both your documents and the query using OpenAIEmbeddings.\n",
    "\n",
    "Similarity Search: You compare the embedded_query against the document embeddings (using cosine similarity or other search techniques) to find the most relevant documents.\n",
    "\n",
    "Nugen API for Completion: Once you have the relevant document(s), you send a prompt to Nugen’s completion model to generate an answer or continue a conversation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
