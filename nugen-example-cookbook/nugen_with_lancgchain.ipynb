{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Integrating Nugen’s Completion Model with LangChain’s Embedding Model**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This guide demonstrates how to combine the strengths of Nugen’s completion API with LangChain’s embedding model to build a text-based application. You can leverage LangChain's embedding capabilities for document storage and retrieval, while using Nugen's completion model for generating natural language outputs based on those embeddings.\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "* **Nugen Completion Model**: An API that generates text completions based on input prompts.\n",
    "* **LangChain Embedding Model**: A framework used for generating text embeddings and storing them in vector stores for retrieval.\n",
    "* **Embeddings**: Numerical representations of text for semantic understanding.\n",
    "* **Vector Store**: A data structure that stores embeddings for fast retrieval.\n",
    "* **Completions**: Generated text responses based on a given input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Set Up the Environment\n",
    "\n",
    "Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\parimal\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet requests langchain PyMuPDF langchain-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Extract Text from PDF\n",
    "We start by extracting text from a PDF file using PyMuPDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Split Text into Chunks\n",
    "\n",
    "Since LangChain’s embedding models work better on smaller chunks of text, split the extracted text into manageable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Splits the extracted text into smaller chunks for embedding.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full text to split.\n",
    "        chunk_size (int): The maximum size of each chunk in characters.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Generate Embeddings Using LangChain\n",
    "\n",
    "Here we use LangChain’s embedding model to generate embeddings for the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Use OpenAI embeddings (or any supported embedding model)\n",
    "def generate_langchain_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    Generates embeddings using LangChain's embedding model.\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): A list of text chunks.\n",
    "        \n",
    "    Returns:\n",
    "        list: Embedding vectors for each text chunk.\n",
    "    \"\"\"\n",
    "    embedding_model = OpenAIEmbeddings()  # Use OpenAI's embedding model\n",
    "    embeddings = [embedding_model.embed(text_chunk) for text_chunk in text_chunks]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Create a Vector Store\n",
    "The embeddings will be stored in a vector store using LangChain’s FAISS implementation, which allows for fast similarity-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def create_langchain_vector_store(text_chunks):\n",
    "    \"\"\"\n",
    "    Creates a vector store using LangChain and the generated embeddings.\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): List of text chunks to embed.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS: A FAISS-based vector store with the embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = generate_langchain_embeddings(text_chunks)\n",
    "    vector_store = FAISS.from_texts(text_chunks, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6**: Use Nugen’s Completion Model for Generating Responses\n",
    "\n",
    "Now, you can use Nugen’s completion API to generate responses for specific queries. You can query the vector store for relevant chunks, and based on those, generate completions using Nugen’s model.\n",
    "\n",
    "**Generate Completion Using Nugen:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_nugen_completion(prompt, max_tokens=400, model=\"nugen-flash-instruct\", temperature=1):\n",
    "    \"\"\"\n",
    "    Calls Nugen's completion API to generate a text response based on a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt for generating the completion.\n",
    "        max_tokens (int): Maximum number of tokens in the response.\n",
    "        model (str): The model to use for completions (default is \"nugen-flash-instruct\").\n",
    "        temperature (float): Sampling temperature (default is 1 for randomization).\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text completion.\n",
    "    \"\"\"\n",
    "    url = \"https://api.nugen.in/inference/completions\"\n",
    "    \n",
    "    payload = {\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer <your_api_token>\",  # Replace with your Nugen API token\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('completion')\n",
    "    else:\n",
    "        raise Exception(f\"Error generating completion: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7**: Query the Vector Store and Generate a Response\n",
    "\n",
    "Now that we have the vector store and Nugen’s completion model integrated, let’s retrieve the most relevant chunks and use them to generate a completion with Nugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_and_generate_response(query, vector_store):\n",
    "    \"\"\"\n",
    "    Queries the vector store and generates a completion using Nugen's API.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query to search for in the document.\n",
    "        vector_store (FAISS): The vector store containing embeddings.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated response based on the most relevant chunks.\n",
    "    \"\"\"\n",
    "    # Query the vector store to get relevant chunks\n",
    "    relevant_chunks = vector_store.similarity_search(query)\n",
    "    \n",
    "    # Combine the most relevant chunks to create the input prompt for Nugen\n",
    "    combined_text = \" \".join([chunk for chunk in relevant_chunks])\n",
    "    \n",
    "    # Use Nugen's completion model to generate a response based on the combined text\n",
    "    completion = get_nugen_completion(combined_text)\n",
    "    \n",
    "    return completion   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8**: Complete Process from PDF to Query and Response\n",
    "\n",
    "Here’s the full process, from extracting text from a PDF to generating a response using LangChain’s embeddings and Nugen’s completion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'sample.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23444\\433886504.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mpdf_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sample.pdf\"\u001b[0m  \u001b[1;31m# Replace with your PDF path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"What are the main benefits of LangChain?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Process the PDF and get a generated response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mgenerated_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_pdf_and_generate_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Output the response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mGenerated Response: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mgenerated_response\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23444\\433886504.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(pdf_path, query)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Step 1: Extract text from the PDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Step 2: Split the text into chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtext_chunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_text_into_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23444\\946574286.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mextracted\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpage_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pymupdf\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[0;32m   2902\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count_pdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2903\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count_fz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mJM_mupdf_show_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJM_mupdf_show_errors_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: no such file: 'sample.pdf'"
     ]
    }
   ],
   "source": [
    "def process_pdf_and_generate_response(pdf_path, query):\n",
    "    \"\"\"\n",
    "    Processes a PDF by extracting text, generating embeddings, and querying the vector store.\n",
    "    Uses Nugen's completion model to generate a response based on the query.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        query (str): The user query to search for.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated response based on the query.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract text from the PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Step 2: Split the text into chunks\n",
    "    text_chunks = split_text_into_chunks(text)\n",
    "    \n",
    "    # Step 3: Create a vector store using LangChain\n",
    "    vector_store = create_langchain_vector_store(text_chunks)\n",
    "    \n",
    "    # Step 4: Query the vector store and generate a response using Nugen\n",
    "    response = query_and_generate_response(query, vector_store)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"legal_service_authorities_act_1987.pdf\"  # Replace with your PDF path\n",
    "query = \"What should the Central Authority consist of?\"\n",
    "\n",
    "# Process the PDF and get a generated response\n",
    "generated_response = process_pdf_and_generate_response(pdf_path, query)\n",
    "\n",
    "# Output the response\n",
    "print(f\"Generated Response: {generated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "By combining LangChain’s embedding model with Nugen’s completion model, we can build a robust system that allows for:\n",
    "\n",
    "Fast and accurate document retrieval using embeddings.\n",
    "Natural language completions based on the retrieved content, enhancing the user experience in applications like Q&A systems, chatbots, and document summarization tools.\n",
    "This integration highlights the power of using Nugen’s completion API for text generation and LangChain for handling text embeddings and retrieval. With these tools, developers can create intelligent, scalable applications that meet diverse NLP use cases.\n",
    "\n",
    "Nugen provides cutting-edge language models that are easy to integrate into modern NLP workflows, while LangChain offers the flexibility to build more complex, multi-step language processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
