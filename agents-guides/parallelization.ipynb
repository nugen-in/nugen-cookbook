{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention! To learn more, visit [Nugen](https://docs.nugen.in/introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM Parallelization**\n",
    "\n",
    "**Understanding LLM Parallelization**\n",
    "\n",
    "**What is LLM Parallelization?**\n",
    "\n",
    "LLM parallelization is like having multiple experts work on the same problem simultaneously rather than asking one expert sequentially. Instead of waiting for one language model to process and respond before moving to the next, we send requests to multiple models concurrently and then combine their insights.\n",
    "\n",
    "**Why Use Parallelization?**\n",
    "\n",
    "1. **Diversity of Responses**: Different models may approach problems differently, providing varied perspectives and insights.\n",
    "\n",
    "2. **Improved Reliability**: By aggregating multiple responses, we can reduce the impact of any single model's biases or errors.\n",
    "\n",
    "3. **Enhanced Performance**: Parallel processing can significantly reduce total response time compared to sequential processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Code Analysis: ParallelLLMProcessor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Overview**\n",
    "\n",
    "The ParallelLLMProcessor class serves as a comprehensive solution for parallel LLM processing. Let's examine each component:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Core Concepts and Architecture**\n",
    "\n",
    "Understanding Parallel Processing in LLMs\n",
    "\n",
    "In traditional LLM processing, we send a query to one model and wait for its response. However, parallel processing allows us to:\n",
    "\n",
    "1. Send the same query to multiple models simultaneously\n",
    "2. Get different perspectives on the same problem\n",
    "3. Combine these perspectives into a more comprehensive answer\n",
    "\n",
    "Think of it like consulting multiple experts at once rather than asking them one at a time. This approach not only saves time but often leads to more well-rounded answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and Installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "from typing import List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1: Key Components and Their Functions**\n",
    "\n",
    "Response Cleaning\n",
    "The response cleaning mechanism is like having an editor who polishes raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize model responses by removing repetition and keeping essential content.\n",
    "    \n",
    "    Args:\n",
    "        response: Raw text response from the language model\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned and normalized response string\n",
    "    \"\"\"\n",
    "    # Split into lines and remove empty ones\n",
    "    lines = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        # Skip questions and non-meaningful content\n",
    "        if (line not in seen and \n",
    "            not line.endswith('?') and \n",
    "            not line.startswith('What') and\n",
    "            not line.startswith('How')):\n",
    "            seen.add(line)\n",
    "            unique_lines.append(line)\n",
    "    \n",
    "    # Take only the first few meaningful lines\n",
    "    relevant_lines = unique_lines[:3]\n",
    "    \n",
    "    return ' '.join(relevant_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This function performs several important cleanup tasks:**\n",
    "\n",
    "- Removes empty lines and extra spaces\n",
    "- Eliminates duplicate content\n",
    "- Filters out questions and redundant text\n",
    "- Keeps only the most relevant information\n",
    "\n",
    "Think of it as a newspaper editor who takes a rough draft and turns it into a concise, clear article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Payload Creation**\n",
    "\n",
    "The payload creation process is like preparing a standardized questionnaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_payload(prompt: str, model: str, system_prompt: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Create the API request payload with appropriate parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The user's input prompt\n",
    "        model: Name of the model to use\n",
    "        system_prompt: Optional system-level instructions\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the formatted API request payload\n",
    "    \"\"\"\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{prompt}\" if system_prompt else prompt\n",
    "    return {\n",
    "        \"max_tokens\": 1000,\n",
    "        \"model\": model,\n",
    "        \"prompt\": full_prompt,\n",
    "        \"temperature\": 0.1,\n",
    "        \"stop\": [\"\\n\\n\", \"###\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: The Parallel Workflow**\n",
    "\n",
    "**Asynchronous LLM Calls**\n",
    "\n",
    "The async functionality is like having multiple phone lines to talk to different experts simultaneously:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_llm_parallel(api_key: str,\n",
    "                          user_prompt: str,\n",
    "                          model: str,\n",
    "                          system_prompt: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Make an asynchronous call to the Nugen API with retry logic and response handling.\n",
    "    \n",
    "    Args:\n",
    "        api_key: Authentication key for the API\n",
    "        user_prompt: The prompt to send to the model\n",
    "        model: Name of the model to use\n",
    "        system_prompt: Optional system-level instructions\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned response from the model\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = create_payload(user_prompt, model, system_prompt)\n",
    "    \n",
    "    for retry_attempt in range(3):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(\n",
    "                    \"https://api.nugen.in/inference/completions\",\n",
    "                    json=payload,\n",
    "                    headers=headers\n",
    "                ) as response:\n",
    "                    # Handle rate limiting\n",
    "                    if response.status == 429:\n",
    "                        wait_time = 2 ** retry_attempt\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                        continue\n",
    "                        \n",
    "                    response.raise_for_status()\n",
    "                    result = await response.json()\n",
    "                    raw_response = result[\"choices\"][0][\"text\"]\n",
    "                    \n",
    "                    return clean_response(raw_response)\n",
    "                    \n",
    "        except aiohttp.ClientError as e:\n",
    "            if retry_attempt == 2:\n",
    "                print(f\"Failed after 3 attempts: {e}\")\n",
    "                raise\n",
    "            await asyncio.sleep(2 ** retry_attempt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function:\n",
    "\n",
    "- Makes non-blocking API calls to the models\n",
    "- Handles retries if a call fails\n",
    "- Implements exponential backoff for rate limits\n",
    "- Processes and cleans the responses\n",
    "\n",
    "Think of it as having an efficient assistant who can make multiple calls at once and handle any communication issues that arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process works like a panel discussion:\n",
    "\n",
    "- Multiple experts (proposer models) receive the same question\n",
    "- They provide their answers independently and simultaneously\n",
    "- A moderator (aggregator model) synthesizes their responses\n",
    "- The final answer combines the best insights from all participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel Workflow**\n",
    "\n",
    "The function acts like a conductor in an orchestra, coordinating multiple AI models to work together on a single task. It first gathers individual perspectives (proposer models) and then combines them into a harmonious final response (aggregator model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parallel_workflow(api_key: str,\n",
    "                          prompt: str,\n",
    "                          proposer_models: List[str],\n",
    "                          aggregator_model: str,\n",
    "                          aggregator_prompt: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Orchestrate parallel LLM calls and aggregate their responses.\n",
    "    \n",
    "    Args:\n",
    "        api_key: Authentication key for the API\n",
    "        prompt: The main prompt to process\n",
    "        proposer_models: List of models to generate initial responses\n",
    "        aggregator_model: Model to use for combining responses\n",
    "        aggregator_prompt: Instructions for aggregating responses\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing (final aggregated response, list of individual responses)\n",
    "    \"\"\"\n",
    "    # Add instructions to prevent repetition\n",
    "    enhanced_prompt = f\"{prompt}\\nProvide a clear, concise answer without repetition.\"\n",
    "    \n",
    "    # Get responses from all proposer models in parallel\n",
    "    proposed_responses = await asyncio.gather(\n",
    "        *[run_llm_parallel(api_key, enhanced_prompt, model) \n",
    "          for model in proposer_models]\n",
    "    )\n",
    "    \n",
    "    # Create enhanced aggregator prompt\n",
    "    enhanced_aggregator_prompt = f\"\"\"\n",
    "    {aggregator_prompt}\n",
    "    Important: Provide a single, clear answer without repeating the question.\n",
    "    Synthesize these responses into one coherent answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the intermediate responses\n",
    "    formatted_responses = \"\\n\".join(\n",
    "        f\"{i+1}. {response}\" \n",
    "        for i, response in enumerate(proposed_responses)\n",
    "    )\n",
    "    \n",
    "    # Get final aggregated response\n",
    "    final_output = await run_llm_parallel(\n",
    "        api_key=api_key,\n",
    "        user_prompt=prompt,\n",
    "        model=aggregator_model,\n",
    "        system_prompt=f\"{enhanced_aggregator_prompt}\\n{formatted_responses}\"\n",
    "    )\n",
    "    \n",
    "    return final_output, proposed_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Purpose: Orchestrates parallel LLM API calls and aggregates multiple model responses into one final output\n",
    "Core Components:**\n",
    "\n",
    "1. Enhances original prompt to prevent repetition\n",
    "2. Runs multiple models in parallel using asyncio.gather\n",
    "3. Aggregates responses using a designated aggregator model\n",
    "\n",
    "\n",
    "**Flow:**\n",
    "\n",
    "1. Creates enhanced prompt with anti-repetition instruction\n",
    "2. Gets parallel responses from proposer models\n",
    "3. Formats intermediate responses numerically\n",
    "4. Generates final aggregated output using aggregator model\n",
    "\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. Asynchronous execution for improved performance\n",
    "2. Structured response formatting\n",
    "3. Two-stage processing (propose â†’ aggregate)\n",
    "4. Returns both final and individual responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Practical Usage and Examples**\n",
    "\n",
    "Here's how to initialize and use the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Response 1:\n",
      "## Step 1: Determine the number of apples Jenna picked. Jenna picked half as many apples as her mom. Since her mom picked 20 apples, Jenna picked 20 / 2 = 10 apples. ## Step 2: Calculate the total number of apples they both picked.\n",
      "\n",
      "Intermediate Response 2:\n",
      "Jenna picked 10 apples. Her mom picked 20 apples. Together they picked 30 apples. The answer is 30.\n",
      "\n",
      "Intermediate Response 3:\n",
      "## Step 1: Determine the number of apples Jenna's mom picked. Jenna's mom picked 20 apples. ## Step 2: Calculate the number of apples Jenna picked.\n",
      "\n",
      "Intermediate Response 4:\n",
      "Use proper grammar and spelling. ## Step 1: Calculate the number of apples Jenna picked. Jenna picked half as many apples as her mom. Since her mom picked 20 apples, Jenna picked 20 / 2 = 10 apples.\n",
      "\n",
      "Final Answer: The answer is 30. The final answer is: $\\boxed{30}$\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    api_key = \"nugen-CnStpNdbBczk3d8SZMhmnw\"\n",
    "    user_prompt = \"\"\"Jenna and her mother picked some apples from their apple farm.\n",
    "    Jenna picked half as many apples as her mom. If her mom got 20 apples, how many apples did they both pick?\"\"\"\n",
    "    \n",
    "    reference_models = [\n",
    "        \"nugen-flash-instruct\",  # Replace with actual Nugen model names\n",
    "        \"llama-v3p2-3b-instruct\",\n",
    "        \"llama-v3p1-8b-instruct\",\n",
    "        \"llama-v3p1-405b-instruct\"\n",
    "    ]\n",
    "    \n",
    "    answer, intermediate_responses = await parallel_workflow(\n",
    "        api_key=api_key,\n",
    "        prompt=user_prompt,\n",
    "        proposer_models=reference_models,\n",
    "        aggregator_model=\"nugen-flash-instruct\",\n",
    "        aggregator_prompt=\"Synthesize these responses into a single clear answer.\"\n",
    "    )\n",
    "    \n",
    "    # Print cleaned results\n",
    "    for i, response in enumerate(intermediate_responses):\n",
    "        print(f\"Intermediate Response {i+1}:\\n{response}\\n\")\n",
    "    print(f\"Final Answer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
