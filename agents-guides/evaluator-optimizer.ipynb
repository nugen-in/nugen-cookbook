{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Nugen Intelligence**\n",
    "<img src=\"https://nugen.in/logo.png\" alt=\"Nugen Logo\" width=\"200\"/>\n",
    "\n",
    "Domain-aligned foundational models at industry leading speeds and zero-data retention! To learn more, visit [Nugen](https://docs.nugen.in/introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Nugen Intelligence Cookbookâ€”a comprehensive guide designed to help you harness the power of domain-aligned large language models (LLMs) with industry-leading speeds and zero-data retention. At Nugen, we specialize in building highly tailored AI models that adapt to your specific needs, enabling scalable, intelligent solutions for specialized domains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluator-Optimizer System Cookbook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "\n",
    "The evaluator-optimizer system is designed to generate and improve code solutions through an iterative process. Think of it as a sophisticated pair programming setup where one AI generates code and another AI reviews it. This cookbook will walk you through each component, explaining how they work and how to use them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports and Installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining Our Evaluation Structure**\n",
    "\n",
    "Next, we create a structure for evaluations, like a grading rubric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(BaseModel):\n",
    "    evaluation: Literal[\"PASS\", \"NEEDS_IMPROVEMENT\", \"FAIL\"]\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is like creating a standardized grading system where:\n",
    "\n",
    "- We have three possible grades (PASS, NEEDS_IMPROVEMENT, FAIL)\n",
    "- Each grade comes with detailed feedback explaining why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Building Our Communication System**\n",
    "\n",
    "The Nugen Client\n",
    "Think of this as our dedicated assistant who knows how to talk to the AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NugenClient:\n",
    "    def __init__(self, token: str):\n",
    "        self.base_url = \"https://api.nugen.in/inference/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client handles:\n",
    "\n",
    "- Setting up the connection (like knowing the phone number and access code)\n",
    "- Maintaining consistent communication settings\n",
    "- Handling any communication errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete method is like having a conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NugenClient:\n",
    "    def __init__(self, token: str):\n",
    "        self.base_url = \"https://api.nugen.in/inference/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def complete(self, prompt: str, model: str = \"nugen-flash-instruct\", \n",
    "                max_tokens: int = 1000, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Makes a completion request to the Nugen API with enhanced error handling\"\"\"\n",
    "        payload = {\n",
    "            \"max_tokens\": str(max_tokens),\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.base_url, json=payload, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"choices\"][0][\"text\"]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API request failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method:\n",
    "\n",
    "- Takes in our question (prompt)\n",
    "- Specifies how detailed we want the answer (max_tokens)\n",
    "- Controls how creative vs focused the response should be (temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Understanding Communication**\n",
    "\n",
    "**Extracting Meaningful Information**\n",
    "\n",
    "Sometimes the AI's responses need interpretation. Our JSON extraction function is like a skilled interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts JSON content from text using various fallback methods.\n",
    "    Includes sophisticated parsing to handle common LLM response patterns.\n",
    "    \"\"\"\n",
    "    # First try to find content between curly braces\n",
    "    json_pattern = r'\\{[^{}]*\\}'\n",
    "    matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "    \n",
    "    for potential_json in matches:\n",
    "        try:\n",
    "            return json.loads(potential_json)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    # If no valid JSON found, try to construct it from the response\n",
    "    try:\n",
    "        # Look for evaluation status\n",
    "        status_pattern = r'(PASS|NEEDS_IMPROVEMENT|FAIL)'\n",
    "        status_match = re.search(status_pattern, text)\n",
    "        \n",
    "        # Look for feedback content\n",
    "        feedback_pattern = r'feedback\"?\\s*:?\\s*\"?([^\"}]*)'\n",
    "        feedback_match = re.search(feedback_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        if status_match and feedback_match:\n",
    "            return {\n",
    "                \"evaluation\": status_match.group(1),\n",
    "                \"feedback\": feedback_match.group(1).strip()\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Default response if no valid JSON or patterns found\n",
    "    return {\n",
    "        \"evaluation\": \"FAIL\",\n",
    "        \"feedback\": \"Could not parse a valid evaluation response\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(user_prompt: str, model: str = \"nugen-flash-instruct\", \n",
    "           system_prompt: str = None, client: NugenClient = None) -> str:\n",
    "    \"\"\"Executes LLM requests with enhanced error handling\"\"\"\n",
    "    if client is None:\n",
    "        raise ValueError(\"NugenClient instance must be provided\")\n",
    "    \n",
    "    full_prompt = f\"{system_prompt}\\n{user_prompt}\" if system_prompt else user_prompt\n",
    "    return client.complete(full_prompt, model=model)\n",
    "\n",
    "def JSON_llm(user_prompt: str, schema, system_prompt: str = None, \n",
    "             client: NugenClient = None) -> dict:\n",
    "    \"\"\"\n",
    "    Executes LLM requests with enhanced JSON handling and response parsing.\n",
    "    Includes multiple fallback methods to ensure valid JSON output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a more explicit JSON instruction with examples\n",
    "        json_instruction = f\"\"\"\n",
    "        Respond ONLY with a valid JSON object following exactly this format:\n",
    "        {{\n",
    "            \"evaluation\": \"PASS\" or \"NEEDS_IMPROVEMENT\" or \"FAIL\",\n",
    "            \"feedback\": \"Your detailed feedback here\"\n",
    "        }}\n",
    "        No other text should be included in your response.\n",
    "        Schema: {json.dumps(schema.model_json_schema())}\n",
    "        \"\"\"\n",
    "        \n",
    "        full_prompt = f\"{json_instruction}\\n{system_prompt}\\n{user_prompt}\" if system_prompt \\\n",
    "                     else f\"{json_instruction}\\n{user_prompt}\"\n",
    "        \n",
    "        response_text = client.complete(full_prompt)\n",
    "        print(f\"\\nRaw evaluation response:\\n{response_text}\\n\")\n",
    "        \n",
    "        # Extract and validate JSON from the response\n",
    "        parsed_response = extract_json_from_text(response_text)\n",
    "        \n",
    "        # Validate against our schema\n",
    "        return Evaluation(**parsed_response).dict()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing evaluation: {str(e)}\")\n",
    "        return {\n",
    "            \"evaluation\": \"FAIL\",\n",
    "            \"feedback\": f\"Error in evaluation process: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works like a detective:\n",
    "\n",
    "- First, it looks for properly formatted JSON\n",
    "- If that fails, it looks for specific patterns\n",
    "- As a last resort, it constructs a response from pieces it finds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 4: The Learning Loop**\n",
    "\n",
    "**Generation and Evaluation**\n",
    "\n",
    "The system works through a cycle of generation and evaluation, like a student practicing problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATOR_PROMPT = \"\"\"\n",
    "Evaluate this code implementation and respond ONLY with a JSON object in this exact format:\n",
    "{\n",
    "    \"evaluation\": \"PASS\" or \"NEEDS_IMPROVEMENT\" or \"FAIL\",\n",
    "    \"feedback\": \"Your detailed feedback here\"\n",
    "}\n",
    "\n",
    "Evaluate based on these criteria:\n",
    "1. code correctness\n",
    "2. time complexity\n",
    "3. style and best practices\n",
    "\n",
    "Use \"PASS\" only if all criteria are met perfectly.\n",
    "Use \"NEEDS_IMPROVEMENT\" if there are minor issues.\n",
    "Use \"FAIL\" if there are major problems.\n",
    "\n",
    "Provide specific, actionable feedback explaining what needs improvement and why.\n",
    "DO NOT include any other text or explanation outside the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "GENERATOR_PROMPT = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback\n",
    "from your previous generations, you should reflect on them to improve your solution.\n",
    "Output your answer concisely in the following format:\n",
    "Thoughts:\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "Response:\n",
    "[Your code implementation here]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Evaluator's Role**\n",
    "\n",
    "The evaluator prompt acts like a thorough teacher who:\n",
    "\n",
    "1. Uses consistent grading criteria\n",
    "2. Provides structured feedback\n",
    "3. Makes clear improvement suggestions\n",
    "\n",
    "**Understanding the Generator's Role**\n",
    "\n",
    "The generator prompt is designed like a thoughtful student who:\n",
    "\n",
    "1. Reads the task carefully\n",
    "2. Considers past feedback\n",
    "3. Plans improvements\n",
    "4. Implements a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(task: str, generator_prompt: str, context: str = \"\", \n",
    "            client: NugenClient = None) -> str:\n",
    "    \"\"\"Generate and improve a solution based on feedback using Nugen API\"\"\"\n",
    "    full_prompt = f\"{generator_prompt}\\n{context}\\nTask: {task}\" if context \\\n",
    "                 else f\"{generator_prompt}\\nTask: {task}\"\n",
    "    \n",
    "    response = run_llm(full_prompt, client=client)\n",
    "    print(\"\\n## Generation start\")\n",
    "    print(f\"Output:\\n{response}\\n\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The generation phase:**\n",
    "\n",
    "1. Takes in a task and any previous context\n",
    "2. Creates a solution\n",
    "3. Returns the attempt for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(task: str, evaluator_prompt: str, generated_content: str, \n",
    "            client: NugenClient = None) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements using Nugen API\"\"\"\n",
    "    full_prompt = f\"{evaluator_prompt}\\nOriginal task: {task}\\n\" + \\\n",
    "                 f\"Content to evaluate: {generated_content}\"\n",
    "    \n",
    "    response = JSON_llm(full_prompt, Evaluation, client=client)\n",
    "    evaluation = response[\"evaluation\"]\n",
    "    feedback = response[\"feedback\"]\n",
    "    \n",
    "    print(\"## Evaluation start\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    return evaluation, feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The evaluation phase:**\n",
    "\n",
    "1. Reviews the generated solution\n",
    "2. Checks it against requirements\n",
    "3. Provides specific feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 5: The Complete Workflow**\n",
    "\n",
    "The main workflow ties everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_workflow(task: str, evaluator_prompt: str, generator_prompt: str, \n",
    "                 token: str, max_iterations: int = 5) -> str:\n",
    "    \"\"\"Execute the main workflow loop with enhanced error handling\"\"\"\n",
    "    client = NugenClient(token)\n",
    "    memory = []\n",
    "    \n",
    "    try:\n",
    "        # Generate initial response\n",
    "        response = generate(task, generator_prompt, client=client)\n",
    "        memory.append(response)\n",
    "        \n",
    "        current_iteration = 0\n",
    "        while current_iteration < max_iterations:\n",
    "            evaluation, feedback = evaluate(task, evaluator_prompt, response, client=client)\n",
    "            \n",
    "            if evaluation == \"PASS\":\n",
    "                return response\n",
    "                \n",
    "            context = \"\\n\".join([\n",
    "                \"Previous attempts:\",\n",
    "                *[f\"- {m}\" for m in memory],\n",
    "                f\"\\nFeedback: {feedback}\"\n",
    "            ])\n",
    "            \n",
    "            response = generate(task, generator_prompt, context, client=client)\n",
    "            memory.append(response)\n",
    "            current_iteration += 1\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Workflow error: {str(e)}\")\n",
    "        return f\"Error in workflow: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This process works like a tutoring session:**\n",
    "\n",
    "1. The system attempts a solution\n",
    "2. Evaluates its work\n",
    "3. If it's not perfect, it tries again with the feedback in mind\n",
    "4. This continues until either:\n",
    "    - A perfect solution is found\n",
    "    - The maximum number of attempts is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 6: Using the System**\n",
    "\n",
    "To get started with Nugen LLM models, you'll need an API key. Here's how to mention it in the cookbook:\n",
    "\n",
    "\"To access Nugen's LLM capabilities, sign up at [Nugen](https://nugen-platform-frontend.azurewebsites.net/dashboard) \n",
    "Obtain your API key from the dashboard. Your API key should be in the format: nugen-xxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "Here's how to use the complete system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Generation start\n",
      "Output:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thoughts:\n",
      "    To implement a stack with O(1) push, pop, and getMin operations, we can use two stacks: one for storing the actual elements and another for storing the minimum elements seen so far. The second stack will be used to keep track of the minimum element at each step.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "\n",
      "    def __init__(self):\n",
      "        \"\"\"\n",
      "        initialize your data structure here.\n",
      "        \"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack:\n",
      "            if self.stack[-1] == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "            self.stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        if self.stack:\n",
      "            return self.stack[-1]\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "```\n",
      "This implementation ensures that all operations (push, pop, getMin) are performed in constant time, O(1). The `min_stack` is used to keep track of the minimum element at each step, allowing us to retrieve the minimum element in O(1) time.\n",
      "\n",
      "\n",
      "Raw evaluation response:\n",
      " The `stack` is used to store the actual elements.\n",
      "\n",
      "```python\n",
      "# Example usage:\n",
      "minStack = MinStack()\n",
      "minStack.push(-2)\n",
      "minStack.push(0)\n",
      "minStack.push(-3)\n",
      "print(minStack.getMin())  # Output: -3\n",
      "minStack.pop()\n",
      "print(minStack.top())     # Output: 0\n",
      "print(minStack.getMin())  # Output: -2\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "    \"evaluation\": \"PASS\",\n",
      "    \"feedback\": \"The implementation is correct, efficient, and follows best practices. The use of two stacks to keep track of the minimum element at each step is a good approach. The code is well-structured, readable, and includes example usage. The time complexity of all operations is O(1) as required.\"\n",
      "}\n",
      "\n",
      "## Evaluation start\n",
      "Status: PASS\n",
      "Feedback: The implementation is correct, efficient, and follows best practices. The use of two stacks to keep track of the minimum element at each step is a good approach. The code is well-structured, readable, and includes example usage. The time complexity of all operations is O(1) as required.\n",
      "\n",
      "Final Result: \n",
      "\n",
      "\n",
      "\n",
      "Thoughts:\n",
      "    To implement a stack with O(1) push, pop, and getMin operations, we can use two stacks: one for storing the actual elements and another for storing the minimum elements seen so far. The second stack will be used to keep track of the minimum element at each step.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "\n",
      "    def __init__(self):\n",
      "        \"\"\"\n",
      "        initialize your data structure here.\n",
      "        \"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack:\n",
      "            if self.stack[-1] == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "            self.stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        if self.stack:\n",
      "            return self.stack[-1]\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "```\n",
      "This implementation ensures that all operations (push, pop, getMin) are performed in constant time, O(1). The `min_stack` is used to keep track of the minimum element at each step, allowing us to retrieve the minimum element in O(1) time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parimal\\AppData\\Local\\Temp\\ipykernel_9136\\1789596196.py:38: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  return Evaluation(**parsed_response).dict()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    task = \"\"\"\n",
    "    Implement a Stack with:\n",
    "    1. push(x)\n",
    "    2. pop()\n",
    "    3. getMin()\n",
    "    All operations should be O(1).\n",
    "    \"\"\"\n",
    "    \n",
    "    token = <your api key> # Replace with your actual token\n",
    "    \n",
    "    try:\n",
    "        result = loop_workflow(task, EVALUATOR_PROMPT, GENERATOR_PROMPT, token)\n",
    "        print(\"\\nFinal Result:\", result)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "This self-improving LLM system demonstrates how we can create AI systems that:\n",
    "\n",
    "Generate solutions to problems\n",
    "- Evaluate their own work\n",
    "- Learn from feedback\n",
    "- Progressively improve their responses\n",
    "\n",
    "The system combines several sophisticated concepts:\n",
    "\n",
    "Structured data validation\n",
    "- Error handling\n",
    "- Pattern matching\n",
    "- Iterative improvement\n",
    "\n",
    "By understanding each component and how they work together, you can create robust systems that not only solve problems but learn and improve from experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
